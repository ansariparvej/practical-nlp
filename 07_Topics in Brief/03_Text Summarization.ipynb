{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "**Text summarization** refers to the task of creating a summary of a longer piece of text. The goal of this task is to create a coherent summary that captures the key ideas in the text.\n",
    "\n",
    "Some of the subtasks:\n",
    "- Extractive versus abstractive summarization\n",
    "\n",
    "Extractive summarization refers to selecting important sentences from a piece of text and showing them together as a summary. Abstractive summarization refers to the task of generating an abstract of the text; i.e., instead of picking sentences from within the text, a new summary is generated.\n",
    "\n",
    "- Query-focused versus query-independent summarization\n",
    "\n",
    "Query-focused summarization refers to creating the summary of the text depending on the user query, whereas query-independent summarization creates a general summary.\n",
    "\n",
    "- Single-document versus multi-document summarization\n",
    "\n",
    "As the names indicate, single-document summarization is the task of creating a summary from a single document, whereas multi-document summarization creates a summary from a collection of documents.\n",
    "\n",
    "> The most common use case for text summarization is a single-document, query-independent, extractive summarization\n",
    "\n",
    "> Popular extractive summarization algorithms used in real-world scenarios use a graph-based sentence-ranking approach.\n",
    "\n",
    "# Practical Advice\n",
    "\n",
    "- Pre-processing steps like sentence splitting (or HTML parsing in the above example) play a very important role in what comes out as output summary.\n",
    "- Most summarization algorithms are sensitive to the size of the text given as input.\n",
    "\n",
    "> TIP: Summarizers are sensitive to text length. So, it may make sense to run a summarizer on selected parts of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization with Sumy\n",
    "\n",
    "Sumy offers several algorithms and methods:\n",
    "1. Luhn – Heurestic method\n",
    "2. Latent Semantic Analysis\n",
    "4. LexRank – Unsupervised approach inspired by algorithms PageRank and HITS\n",
    "5. TextRank - Graph-based summarization technique with keyword extractions in from document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yasir Abdur\n",
      "[nltk_data]     Rohman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRankSummarizer:\n",
      "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n",
      "Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
      "------------------------------\n",
      "LexRankSummarizer:\n",
      "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "------------------------------\n",
      "LuhnSummarizer:\n",
      "This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).\n",
      "A Class of Submodular Functions for Document Summarization \", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011 ^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization , In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.\n",
      "------------------------------\n",
      "LsaSummarizer\n",
      "Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper.\n",
      "Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Code to summarize a given webpage using Sumy's TextRank implementation. \n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "num_sentences_in_summary = 2\n",
    "url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n",
    "parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n",
    "\n",
    "summarizer_list=(\"TextRankSummarizer:\",\"LexRankSummarizer:\",\"LuhnSummarizer:\",\"LsaSummarizer\") #list of summarizers\n",
    "summarizers = [TextRankSummarizer(), LexRankSummarizer(), LuhnSummarizer(), LsaSummarizer()]\n",
    "\n",
    "for i,summarizer in enumerate(summarizers):\n",
    "    print(summarizer_list[i])\n",
    "    for sentence in summarizer(parser.document, num_sentences_in_summary):\n",
    "        print((sentence))\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization with gensim\n",
    "\n",
    "> gensim's summarizer uses TextRank by default, an algorithm that uses PageRank. In gensim it is unfortunately implemented using a Python list of PageRank graph nodes, so it may fail if your graph is too big.\n",
    "\n",
    "The two parameters **word_count** and **ratio** we can adjust how much text the summarizer outputs\n",
    "\n",
    "1. word_count: maximum amount of words we want in the summary\n",
    "2. ratio: fraction of sentences in the original text should be returned as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize:\n",
      " Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.\n",
      "This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n",
      "However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
      "Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.\n",
      "In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.\n",
      "------------------------------ \n",
      "Summarize Corpus\n",
      " [[(3, 1), (7, 1), (10, 1), (11, 1), (12, 1), (22, 1), (24, 1), (27, 1), (80, 1), (94, 1), (95, 1), (193, 1), (199, 1), (214, 1), (249, 1), (262, 1), (413, 1), (418, 1), (449, 1), (450, 1), (451, 1), (452, 1), (453, 1), (454, 1), (455, 1), (456, 1), (457, 1), (458, 1), (459, 1), (460, 1), (461, 1), (462, 1), (463, 1), (464, 1)], [(11, 1), (12, 1), (13, 1), (17, 3), (38, 1), (55, 1), (57, 1), (76, 1), (82, 2), (94, 1), (164, 1), (203, 1), (206, 2), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize,summarize_corpus\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim import corpora\n",
    "\n",
    "text = open(\"Data/nlphistory.txt\").read()\n",
    "\n",
    "#summarize method extracts the most relevant sentences in a text\n",
    "print(\"Summarize:\\n\",summarize(text, word_count=200, ratio = 0.1))\n",
    "\n",
    "\n",
    "#the summarize_corpus selects the most important documents in a corpus:\n",
    "sentences = split_sentences(text)# Creates a corpus where each document is a sentence.\n",
    "tokens = [sentence.split() for sentence in sentences]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(sentence_tokens) for sentence_tokens in tokens]\n",
    "\n",
    "# Extracts the most important documents (shown here in BoW representation)\n",
    "print(\"-\"*30,\"\\nSummarize Corpus\\n\",summarize_corpus(corpus,ratio=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summa Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
      "In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[4][5] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[6] parsing,[7][8] and many others.\n"
     ]
    }
   ],
   "source": [
    "from summa import summarizer\n",
    "from summa import keywords\n",
    "text = open(\"Data/nlphistory.txt\").read()\n",
    "\n",
    "print(\"Summary:\")\n",
    "print (summarizer.summarize(text,ratio=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for Extractive Summarization\n",
    "\n",
    "> NOTE: not implemented yet in github practical-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented in this Google Colab\n",
    "# https://colab.research.google.com/drive/1oTKKhf0ZiDZ66nhlkpdR7AS2sbg6gEji?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the required libraries\n",
    "# !pip install bert-extractive-summarizer\n",
    "# !pip install spacy\n",
    "# !pip install transformers\n",
    "# !pip install neuralcoref\n",
    "# # !pip install torch #you can comment this line if u already have tensorflow2.0 installed\n",
    "# !pip install neuralcoref --no-binary neuralcoref\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sowyma could you please look at this coreference vs without coreference. I personally think we need to use a better input.\n",
    "#currently using the same one as above the nlphistory.txt\n",
    "\n",
    "# from summarizer import Summarizer\n",
    "# from summarizer.coreference_handler import CoreferenceHandler\n",
    "\n",
    "# model = Summarizer()\n",
    "\n",
    "# print(\"Without Coreference:\")\n",
    "# result = model(text, min_length=200,ratio=0.01)\n",
    "# full = ''.join(result)\n",
    "# print(full)\n",
    "\n",
    "\n",
    "# print(\"With Coreference:\")\n",
    "# handler = CoreferenceHandler(greedyness=.35)\n",
    "\n",
    "# model = Summarizer(sentence_handler=handler)\n",
    "# result = model(text, min_length=200,ratio=0.01)\n",
    "# full = ''.join(result)\n",
    "# print(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization\n",
    "\n",
    "There have been even efforts to use RL for summarization.\n",
    "The past few years RNNs using encoder — decoder models have become popular for abstractive summarization.\n",
    "Recently Transformers which use attention mechanism have become popular for abstractive summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented in this Google Colab\n",
    "# https://colab.research.google.com/drive/1oTKKhf0ZiDZ66nhlkpdR7AS2sbg6gEji?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# text =\"\"\"\n",
    "# don’t build your own MT system if you don’t have to. It is more practical to make use of the translation APIs. When we use such APIs, it is important to pay closer attention to pricing policies. It would perhaps make sense to store the translations of frequently used text (called a translation memory or a translation cache). \n",
    "\n",
    "# If you’re working with a entirely new language, or say a new domain where existing translation APIs do poorly, it would make sense to start with a domain knowledge based rule based translation system addressing the restricted scenario you deal with. Another approach to address such data scarce scenarios is to augment your training data by doing “back translation”. Let us say we want to translate from English to Navajo language. English is a popular language for MT, but Navajo is not. We do have a few examples of English-Navajo translation. In such a case, one can build a first MT model between Navajo-English, and use this system to translate a few Navajo sentences into English. At this point, these machine translated Navajo-English pairs can be added as additional training data to English-Navajo MT system. This results in a translation system with more examples to train on (even though some of these examples are synthetic). In general, though, if accuracy of translation is paramount, it would perhaps make sense to form a hybrid MT system which combines the neural models with rules and some form of post-processing, though. \n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "# t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "# print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "# tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# # summmarize \n",
    "# summary_ids = model.generate(tokenized_text,\n",
    "#                                     num_beams=4,\n",
    "#                                     no_repeat_ngram_size=2,\n",
    "#                                     min_length=30,\n",
    "#                                     max_length=100,\n",
    "#                                     early_stopping=True)\n",
    "# #there are more parameters which can be found at https://huggingface.co/transformers/model_doc/t5.html\n",
    "\n",
    "# output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
