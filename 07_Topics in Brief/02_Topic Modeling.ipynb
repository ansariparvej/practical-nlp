{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic modeling** generally refers to a collection of unsupervised statistical learning methods to discover latent topics in a large collection of text documents.\n",
    "\n",
    "Some of the popular topic modeling algorithms are latent Dirichlet allocation (LDA), latent semantic analysis (LSA), and probabilistic latent semantic analysis (PLSA). In practice, the technique thatâ€™s most commonly used is LDA.\n",
    "\n",
    "> TIP: Removing words with low frequency or keeping only those words that are nouns and verbs are some ways of improving a topic model. If the corpus is big, divide it into batches of fixed sizes and run topic modeling for each batch. The best output comes from the intersection of topics from each batch.\n",
    "\n",
    "Use case for topic models are:\n",
    "- Summarizing documents, tweets, etc., in the form of keywords based on learned topic distributions\n",
    "- Detecting social media trends over a period of time\n",
    "- Designing recommender systems for text\n",
    "\n",
    "> NOTE: LDA typically work only with long documents and perform poorly on short documents, such as a corpus of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yasir Abdur\n",
      "[nltk_data]     Rohman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.007099087, 'he'),\n",
      "   (0.006641345, 'she'),\n",
      "   (0.0049030785, 'back'),\n",
      "   (0.004803687, 'one'),\n",
      "   (0.004622856, 'tells'),\n",
      "   (0.0046049664, 'they'),\n",
      "   (0.004538162, 'house'),\n",
      "   (0.004194234, 'mother'),\n",
      "   (0.0041599944, 'go'),\n",
      "   (0.0039790394, 'find'),\n",
      "   (0.0038036027, 'when'),\n",
      "   (0.0037479524, 'get'),\n",
      "   (0.0037207666, 'father'),\n",
      "   (0.0036563275, 'two'),\n",
      "   (0.003543434, 'home'),\n",
      "   (0.0034573951, 'after'),\n",
      "   (0.0033719945, 'time'),\n",
      "   (0.003272797, 'day'),\n",
      "   (0.0032547042, 'finds'),\n",
      "   (0.003218834, 'family')],\n",
      "  -0.8686722811252202),\n",
      " ([(0.007681313, 'life'),\n",
      "   (0.0068563065, 'he'),\n",
      "   (0.006769424, 'novel'),\n",
      "   (0.0052444045, 'book'),\n",
      "   (0.005243887, 'family'),\n",
      "   (0.0048457133, 'story'),\n",
      "   (0.004737223, 'one'),\n",
      "   (0.0046171625, 'love'),\n",
      "   (0.0043488615, 'in'),\n",
      "   (0.0042025405, 'also'),\n",
      "   (0.004072716, 'new'),\n",
      "   (0.0038654734, 'young'),\n",
      "   (0.0037804728, 'first'),\n",
      "   (0.0037744278, 'father'),\n",
      "   (0.0035221083, 'two'),\n",
      "   (0.0032744429, 'becomes'),\n",
      "   (0.0030785778, 'time'),\n",
      "   (0.0030668606, 'mother'),\n",
      "   (0.0030465107, 'she'),\n",
      "   (0.0028744233, 'relationship')],\n",
      "  -1.0319218535991357),\n",
      " ([(0.0063218935, 'king'),\n",
      "   (0.00562921, 'he'),\n",
      "   (0.0054028877, 'one'),\n",
      "   (0.0044028386, 'they'),\n",
      "   (0.0041440954, 'find'),\n",
      "   (0.003790675, 'help'),\n",
      "   (0.0036601338, 'she'),\n",
      "   (0.0036120382, 'two'),\n",
      "   (0.003575618, 'back'),\n",
      "   (0.0034239667, 'also'),\n",
      "   (0.003172466, 'father'),\n",
      "   (0.0031505534, 'time'),\n",
      "   (0.003068796, 'way'),\n",
      "   (0.0030402637, 'however'),\n",
      "   (0.0029923974, 'dragon'),\n",
      "   (0.002721912, 'finds'),\n",
      "   (0.0026864226, 'in'),\n",
      "   (0.0026731465, 'kill'),\n",
      "   (0.0026648229, 'return'),\n",
      "   (0.0026456995, 'must')],\n",
      "  -1.1133452273126283),\n",
      " ([(0.006772882, 'he'),\n",
      "   (0.0061983797, 'one'),\n",
      "   (0.0050678332, 'back'),\n",
      "   (0.004067628, 'world'),\n",
      "   (0.003948651, 'she'),\n",
      "   (0.0036912605, 'find'),\n",
      "   (0.0034169941, 'they'),\n",
      "   (0.0032712508, 'also'),\n",
      "   (0.0032334097, 'after'),\n",
      "   (0.003215792, 'in'),\n",
      "   (0.0031959885, 'city'),\n",
      "   (0.0031025829, 'tells'),\n",
      "   (0.003020005, 'new'),\n",
      "   (0.0029811123, 'life'),\n",
      "   (0.0029416739, 'people'),\n",
      "   (0.0028902553, 'two'),\n",
      "   (0.0027891663, 'vampire'),\n",
      "   (0.0026629826, 'when'),\n",
      "   (0.002578822, 'book'),\n",
      "   (0.0025493973, 'first')],\n",
      "  -1.1627012101252678),\n",
      " ([(0.0071979323, 'war'),\n",
      "   (0.0049560415, 'world'),\n",
      "   (0.0042760842, 'new'),\n",
      "   (0.0041000475, 'ship'),\n",
      "   (0.0040743323, 'one'),\n",
      "   (0.0038249858, 'in'),\n",
      "   (0.0033914808, 'also'),\n",
      "   (0.0033167843, 'time'),\n",
      "   (0.003219057, 'novel'),\n",
      "   (0.0032179828, 'book'),\n",
      "   (0.0031941729, 'first'),\n",
      "   (0.0030763885, 'story'),\n",
      "   (0.0030268496, 'two'),\n",
      "   (0.0027287211, 'he'),\n",
      "   (0.0024548254, 'crew'),\n",
      "   (0.0024496233, 'group'),\n",
      "   (0.002417316, 'army'),\n",
      "   (0.002384212, 'a'),\n",
      "   (0.0023095517, 'forces'),\n",
      "   (0.002306066, 'states')],\n",
      "  -1.4097704633863577),\n",
      " ([(0.0068261353, 'earth'),\n",
      "   (0.005698468, 'one'),\n",
      "   (0.005634491, 'he'),\n",
      "   (0.0049907323, 'planet'),\n",
      "   (0.0042311284, 'dresden'),\n",
      "   (0.004146917, 'space'),\n",
      "   (0.0040026098, 'ship'),\n",
      "   (0.0038170333, 'time'),\n",
      "   (0.0037036035, 'they'),\n",
      "   (0.0034382169, 'new'),\n",
      "   (0.003217138, 'human'),\n",
      "   (0.0030635183, 'however'),\n",
      "   (0.0029894821, 'back'),\n",
      "   (0.0028177826, 'also'),\n",
      "   (0.0027744474, 'in'),\n",
      "   (0.0026642764, 'billy'),\n",
      "   (0.002563283, 'after'),\n",
      "   (0.0025381655, 'help'),\n",
      "   (0.0024636353, 'control'),\n",
      "   (0.002399433, 'take')],\n",
      "  -1.6292076834393618),\n",
      " ([(0.0058817905, 'story'),\n",
      "   (0.0050666286, 'man'),\n",
      "   (0.0047909706, 'she'),\n",
      "   (0.0047614374, 'he'),\n",
      "   (0.004610612, 'father'),\n",
      "   (0.0045844307, 'one'),\n",
      "   (0.0043592146, 'mother'),\n",
      "   (0.003983345, 'time'),\n",
      "   (0.003699208, 'henry'),\n",
      "   (0.0036416324, 'harry'),\n",
      "   (0.0033654252, 'old'),\n",
      "   (0.0033217405, 'murder'),\n",
      "   (0.0032005194, 'alice'),\n",
      "   (0.0031796335, 'tells'),\n",
      "   (0.0030306517, 'i'),\n",
      "   (0.002972666, 'monk'),\n",
      "   (0.0029031946, 'in'),\n",
      "   (0.0028517775, 'wife'),\n",
      "   (0.002843803, 'also'),\n",
      "   (0.0027725135, 'two')],\n",
      "  -1.7425638462971016),\n",
      " ([(0.006540102, 'jacky'),\n",
      "   (0.005951149, 'one'),\n",
      "   (0.0058514117, 'book'),\n",
      "   (0.004447075, 'max'),\n",
      "   (0.0042199725, 'he'),\n",
      "   (0.0038028886, 'in'),\n",
      "   (0.0037409978, 'also'),\n",
      "   (0.0034728276, 'new'),\n",
      "   (0.0034640862, 'people'),\n",
      "   (0.0033816595, 'time'),\n",
      "   (0.0033368114, 'human'),\n",
      "   (0.0033317858, 'school'),\n",
      "   (0.0032204103, 'life'),\n",
      "   (0.0030035924, 'world'),\n",
      "   (0.0027472028, 'chapter'),\n",
      "   (0.0026911872, 'this'),\n",
      "   (0.0025355036, 'story'),\n",
      "   (0.002457422, 'first'),\n",
      "   (0.0022389174, 'two'),\n",
      "   (0.002025653, 'jason')],\n",
      "  -1.8054070732519423),\n",
      " ([(0.0071633905, 'paul'),\n",
      "   (0.006823348, 'richard'),\n",
      "   (0.0066169966, 'he'),\n",
      "   (0.0051746466, 'father'),\n",
      "   (0.004742662, 'one'),\n",
      "   (0.004031633, 'new'),\n",
      "   (0.003998363, 'book'),\n",
      "   (0.0039550713, 'mother'),\n",
      "   (0.0037775273, 'story'),\n",
      "   (0.0035230613, 'matt'),\n",
      "   (0.0034888606, 'nora'),\n",
      "   (0.003487911, 'also'),\n",
      "   (0.0034056136, 'man'),\n",
      "   (0.0033395928, 'school'),\n",
      "   (0.003338544, 'back'),\n",
      "   (0.0032285228, 'william'),\n",
      "   (0.0031596143, 'todd'),\n",
      "   (0.0031247842, 'brooke'),\n",
      "   (0.0030299735, 'life'),\n",
      "   (0.0030117568, 'home')],\n",
      "  -3.069885522449467),\n",
      " ([(0.0056600613, 'love'),\n",
      "   (0.0052917325, 'he'),\n",
      "   (0.0050991685, 'arthur'),\n",
      "   (0.005033334, 'vlad'),\n",
      "   (0.005020884, 'chris'),\n",
      "   (0.0048250207, 'one'),\n",
      "   (0.004355915, 'hugh'),\n",
      "   (0.0041024764, 'she'),\n",
      "   (0.003968751, 'in'),\n",
      "   (0.0039278297, 'peter'),\n",
      "   (0.0037492325, 'olivia'),\n",
      "   (0.0036393148, 'maya'),\n",
      "   (0.0034567888, 'father'),\n",
      "   (0.0033960843, 'also'),\n",
      "   (0.0033145878, 'laura'),\n",
      "   (0.003297008, 'adam'),\n",
      "   (0.0032303308, 'first'),\n",
      "   (0.0031900848, 'guy'),\n",
      "   (0.003047714, 'harry'),\n",
      "   (0.0030291688, 'after')],\n",
      "  -3.9885888967137118)]\n"
     ]
    }
   ],
   "source": [
    "#tokenize, remove stopwords, non-alphabetic words, lowercase\n",
    "def preprocess(textstring):\n",
    "    stops =  set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(textstring)\n",
    "    return [token.lower() for token in tokens if token.isalpha() and token not in stops]\n",
    "\n",
    "data_path = \"Data/booksummaries/booksummaries.txt\"\n",
    "summaries = []\n",
    "for line in open(data_path, encoding=\"utf-8\"):\n",
    "    temp = line.split(\"\\t\")\n",
    "    summaries.append(preprocess(temp[6]))\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(summaries)\n",
    "# Filter infrequent or too frequent words.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(summary) for summary in summaries]\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "#Train the topic model\n",
    "model = LdaModel(corpus=corpus, id2word=id2word,iterations=400, num_topics=10)\n",
    "top_topics = list(model.top_topics(corpus))\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.007*\"paul\" + 0.007*\"richard\" + 0.007*\"he\" + 0.005*\"father\" + 0.005*\"one\" + 0.004*\"new\" + 0.004*\"book\" + 0.004*\"mother\" + 0.004*\"story\" + 0.004*\"matt\"\n",
      "Topic #1: 0.007*\"he\" + 0.007*\"she\" + 0.005*\"back\" + 0.005*\"one\" + 0.005*\"tells\" + 0.005*\"they\" + 0.005*\"house\" + 0.004*\"mother\" + 0.004*\"go\" + 0.004*\"find\"\n",
      "Topic #2: 0.006*\"love\" + 0.005*\"he\" + 0.005*\"arthur\" + 0.005*\"vlad\" + 0.005*\"chris\" + 0.005*\"one\" + 0.004*\"hugh\" + 0.004*\"she\" + 0.004*\"in\" + 0.004*\"peter\"\n",
      "Topic #3: 0.007*\"war\" + 0.005*\"world\" + 0.004*\"new\" + 0.004*\"ship\" + 0.004*\"one\" + 0.004*\"in\" + 0.003*\"also\" + 0.003*\"time\" + 0.003*\"novel\" + 0.003*\"book\"\n",
      "Topic #4: 0.007*\"jacky\" + 0.006*\"one\" + 0.006*\"book\" + 0.004*\"max\" + 0.004*\"he\" + 0.004*\"in\" + 0.004*\"also\" + 0.003*\"new\" + 0.003*\"people\" + 0.003*\"time\"\n",
      "Topic #5: 0.007*\"he\" + 0.006*\"one\" + 0.005*\"back\" + 0.004*\"world\" + 0.004*\"she\" + 0.004*\"find\" + 0.003*\"they\" + 0.003*\"also\" + 0.003*\"after\" + 0.003*\"in\"\n",
      "Topic #6: 0.007*\"earth\" + 0.006*\"one\" + 0.006*\"he\" + 0.005*\"planet\" + 0.004*\"dresden\" + 0.004*\"space\" + 0.004*\"ship\" + 0.004*\"time\" + 0.004*\"they\" + 0.003*\"new\"\n",
      "Topic #7: 0.008*\"life\" + 0.007*\"he\" + 0.007*\"novel\" + 0.005*\"book\" + 0.005*\"family\" + 0.005*\"story\" + 0.005*\"one\" + 0.005*\"love\" + 0.004*\"in\" + 0.004*\"also\"\n",
      "Topic #8: 0.006*\"king\" + 0.006*\"he\" + 0.005*\"one\" + 0.004*\"they\" + 0.004*\"find\" + 0.004*\"help\" + 0.004*\"she\" + 0.004*\"two\" + 0.004*\"back\" + 0.003*\"also\"\n",
      "Topic #9: 0.006*\"story\" + 0.005*\"man\" + 0.005*\"she\" + 0.005*\"he\" + 0.005*\"father\" + 0.005*\"one\" + 0.004*\"mother\" + 0.004*\"time\" + 0.004*\"henry\" + 0.004*\"harry\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, model.print_topic(idx, 10))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.305*\"he\" + 0.215*\"one\" + 0.150*\"she\" + 0.140*\"time\" + 0.132*\"back\" + '\n",
      "  '0.131*\"also\" + 0.127*\"two\" + 0.125*\"they\" + 0.123*\"tells\" + 0.118*\"in\"'),\n",
      " (1,\n",
      "  '-0.493*\"tom\" + -0.226*\"sophia\" + -0.182*\"mrs\" + -0.178*\"house\" + '\n",
      "  '-0.161*\"she\" + -0.154*\"father\" + -0.147*\"mr\" + -0.146*\"he\" + -0.138*\"tells\" '\n",
      "  '+ 0.126*\"one\"'),\n",
      " (2,\n",
      "  '-0.558*\"tom\" + -0.252*\"sophia\" + 0.213*\"she\" + 0.191*\"he\" + -0.185*\"mrs\" + '\n",
      "  '0.163*\"tells\" + 0.143*\"mother\" + -0.136*\"mr\" + -0.130*\"western\" + '\n",
      "  '-0.103*\"however\"'),\n",
      " (3,\n",
      "  '-0.233*\"they\" + -0.203*\"ship\" + 0.186*\"he\" + -0.183*\"david\" + -0.182*\"back\" '\n",
      "  '+ -0.163*\"tells\" + 0.161*\"life\" + 0.161*\"family\" + -0.155*\"find\" + '\n",
      "  '0.154*\"narrator\"'),\n",
      " (4,\n",
      "  '0.663*\"he\" + -0.257*\"mother\" + -0.213*\"she\" + -0.195*\"father\" + '\n",
      "  '-0.181*\"family\" + 0.121*\"narrator\" + 0.120*\"monk\" + -0.098*\"novel\" + '\n",
      "  '-0.097*\"school\" + -0.095*\"children\"'),\n",
      " (5,\n",
      "  '0.486*\"david\" + -0.244*\"king\" + 0.169*\"rosa\" + 0.163*\"book\" + '\n",
      "  '0.125*\"harlan\" + -0.121*\"he\" + 0.113*\"she\" + -0.111*\"anita\" + 0.107*\"gould\" '\n",
      "  '+ 0.104*\"would\"'),\n",
      " (6,\n",
      "  '-0.697*\"anita\" + -0.472*\"richard\" + 0.158*\"ship\" + 0.130*\"jacky\" + '\n",
      "  '-0.084*\"edward\" + -0.082*\"power\" + -0.081*\"monk\" + -0.071*\"scene\" + '\n",
      "  '-0.071*\"kill\" + 0.068*\"father\"'),\n",
      " (7,\n",
      "  '-0.398*\"david\" + -0.354*\"king\" + 0.224*\"jacky\" + 0.195*\"ship\" + '\n",
      "  '0.144*\"monk\" + 0.135*\"doctor\" + -0.128*\"rosa\" + -0.125*\"arthur\" + '\n",
      "  '-0.122*\"prince\" + -0.110*\"book\"'),\n",
      " (8,\n",
      "  '-0.284*\"harry\" + 0.281*\"she\" + -0.267*\"narrator\" + -0.231*\"david\" + '\n",
      "  '0.220*\"jacky\" + -0.194*\"monk\" + -0.142*\"natalie\" + 0.130*\"ship\" + '\n",
      "  '0.124*\"says\" + 0.117*\"king\"'),\n",
      " (9,\n",
      "  '0.453*\"harry\" + -0.410*\"narrator\" + 0.269*\"monk\" + -0.216*\"david\" + '\n",
      "  '-0.214*\"anita\" + 0.187*\"natalie\" + -0.163*\"he\" + -0.163*\"ship\" + '\n",
      "  '-0.154*\"richard\" + 0.149*\"dresden\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsamodel = LsiModel(corpus, num_topics=10, id2word = id2word)  # train model\n",
    "\n",
    "pprint(lsamodel.print_topics(num_topics=10, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.305*\"he\" + 0.215*\"one\" + 0.150*\"she\" + 0.140*\"time\" + 0.132*\"back\" + 0.131*\"also\" + 0.127*\"two\" + 0.125*\"they\" + 0.123*\"tells\" + 0.118*\"in\"\n",
      "Topic #1: -0.493*\"tom\" + -0.226*\"sophia\" + -0.182*\"mrs\" + -0.178*\"house\" + -0.161*\"she\" + -0.154*\"father\" + -0.147*\"mr\" + -0.146*\"he\" + -0.138*\"tells\" + 0.126*\"one\"\n",
      "Topic #2: -0.558*\"tom\" + -0.252*\"sophia\" + 0.213*\"she\" + 0.191*\"he\" + -0.185*\"mrs\" + 0.163*\"tells\" + 0.143*\"mother\" + -0.136*\"mr\" + -0.130*\"western\" + -0.103*\"however\"\n",
      "Topic #3: -0.233*\"they\" + -0.203*\"ship\" + 0.186*\"he\" + -0.183*\"david\" + -0.182*\"back\" + -0.163*\"tells\" + 0.161*\"life\" + 0.161*\"family\" + -0.155*\"find\" + 0.154*\"narrator\"\n",
      "Topic #4: 0.663*\"he\" + -0.257*\"mother\" + -0.213*\"she\" + -0.195*\"father\" + -0.181*\"family\" + 0.121*\"narrator\" + 0.120*\"monk\" + -0.098*\"novel\" + -0.097*\"school\" + -0.095*\"children\"\n",
      "Topic #5: 0.486*\"david\" + -0.244*\"king\" + 0.169*\"rosa\" + 0.163*\"book\" + 0.125*\"harlan\" + -0.121*\"he\" + 0.113*\"she\" + -0.111*\"anita\" + 0.107*\"gould\" + 0.104*\"would\"\n",
      "Topic #6: -0.697*\"anita\" + -0.472*\"richard\" + 0.158*\"ship\" + 0.130*\"jacky\" + -0.084*\"edward\" + -0.082*\"power\" + -0.081*\"monk\" + -0.071*\"scene\" + -0.071*\"kill\" + 0.068*\"father\"\n",
      "Topic #7: -0.398*\"david\" + -0.354*\"king\" + 0.224*\"jacky\" + 0.195*\"ship\" + 0.144*\"monk\" + 0.135*\"doctor\" + -0.128*\"rosa\" + -0.125*\"arthur\" + -0.122*\"prince\" + -0.110*\"book\"\n",
      "Topic #8: -0.284*\"harry\" + 0.281*\"she\" + -0.267*\"narrator\" + -0.231*\"david\" + 0.220*\"jacky\" + -0.194*\"monk\" + -0.142*\"natalie\" + 0.130*\"ship\" + 0.124*\"says\" + 0.117*\"king\"\n",
      "Topic #9: 0.453*\"harry\" + -0.410*\"narrator\" + 0.269*\"monk\" + -0.216*\"david\" + -0.214*\"anita\" + 0.187*\"natalie\" + -0.163*\"he\" + -0.163*\"ship\" + -0.154*\"richard\" + 0.149*\"dresden\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, lsamodel.print_topic(idx, 10))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
