{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Models\n",
    "Representation text units (characters, phonemes, words, phrases, sentences, paragraphs, and documents) with vector of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Vectorization Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding, cons:\n",
    "1. The size of one-hot vector is directly proportional to size of the vocabulary, and most real-world corpora have large vocabularies. This results in a sparse representation.\n",
    "2. This representation does not give a fixed-length representation for text, i.e., if a text has 10 words, you get longer representation for it as compared to a text with 5 words.\n",
    "3. It treats words as atomic units and has no notion of (dis)similarity between words. Semantically, very poor at capturing the meaning of the word in relation to other words.\n",
    "4. Cannot handle ouf of vocabulary (OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed docs: ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
      "Vocabulary: {'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n",
      "Docs 1 preprocessed: dog bites man\n",
      "Docs 1 one hot: [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]]\n",
      "One hot random text: [[0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "print(f'Processed docs: {processed_docs}')\n",
    "\n",
    "# build vocabulary\n",
    "vocab = {}\n",
    "count = 0\n",
    "for doc in processed_docs:\n",
    "    for word in doc.split():\n",
    "        if word not in vocab:\n",
    "            count += 1\n",
    "            vocab[word] = count\n",
    "print(f'Vocabulary: {vocab}')\n",
    "\n",
    "# onehot vector\n",
    "def get_onehot_vector(somestring):\n",
    "    onehot_encoded = []\n",
    "    for word in somestring.split():\n",
    "        temp = [0]*len(vocab)\n",
    "        if word in vocab:\n",
    "            temp[vocab[word]-1] = 1  # use -1 because index array starts from 0 not 1\n",
    "        onehot_encoded.append(temp)\n",
    "    return onehot_encoded\n",
    "\n",
    "print(f'Docs 1 preprocessed: {processed_docs[0]}')\n",
    "print(f'Docs 1 one hot: {get_onehot_vector(processed_docs[0])}')\n",
    "\n",
    "print(f'One hot random text: {get_onehot_vector(\"man and dog are good\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words, \n",
    "\n",
    "Advantages:\n",
    "1. Simple to understand and implement\n",
    "2. Captures the semantic similarity of documents. Because documents having the same words will have their vector representations closer to each other in Euclidean space as compared to documents with completely different words.\n",
    "3. A fixed-length encoding for any sentence of arbitrary length\n",
    "\n",
    "Disadvantages:\n",
    "1. The size of the vector increases with the size of the vocabulary. Sparsity problem. One way to control it is by limiting the vocabulary.\n",
    "2. It does not capture the similarity between different words that mean the same thing.\n",
    "3. It does not have any way to handle out of vocabulary words.\n",
    "4. Word order information is lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
      "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
      "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n",
      "\n",
      "BoW with binary vectors:\n",
      "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#look at the documents list\n",
    "print(\"Our corpus: \", processed_docs)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())\n",
    "\n",
    "# Researchers have shown that such a representation without considering frequency is useful for sentiment analysis\n",
    "# BoW with binary vectors\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "bow_rep_bin = count_vect.fit_transform(processed_docs)\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"\\nBoW with binary vectors:\")\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of N-Grams\n",
    "\n",
    "Prons and cons:\n",
    "1. It captures some context and word-order information in the form of n-grams\n",
    "2. The resulting vector space is able to capture some semantic similarity.\n",
    "3. As n increases, dimensionality (and therefore sparsity) only increases rapidly.\n",
    "4. It still provides no way to address the OOV problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
      "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n",
    "\n",
    "It aims to quantify the importance of a given word relative to other words in the document and in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
      "----------\n",
      "All words in the vocabulary ['bites', 'dog', 'eats', 'food', 'man', 'meat']\n",
      "----------\n",
      "TFIDF representation for all documents in our corpus\n",
      " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
      " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
      "----------\n",
      "Tfidf representation for 'dog and man are friends':\n",
      " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
    "\n",
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#TFIDF representation for all documents in our corpus \n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three fundamentals drawback from basic vectorization approaches:\n",
    "1. Discrete representations, it is hampers their ability to capture relationships between words.\n",
    "2. The feature vectors are sparse and high-dimensional representations. The high-dimensionality representations makes them computationally inefficient.\n",
    "3. Cannot handle OOV words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Representations\n",
    "\n",
    "Some key terms:\n",
    "- Distributional similarity, the meaning of the word can be understood from the context (connotation)\n",
    "- Distributional hypothesis, this hypothesizes that words that occur in similar context have similar meanings.\n",
    "- Distributional representation, representation schemes that are obtained based on distribution of words from the context in which the words appear. (one-hot, bag of words, bag of n-grams, TF-IDF)\n",
    "- Distributed representation, is based on the distributional hypothesis.\n",
    "- Embedding, is a mapping between vector space from distributional representation to vector space from distributed representation.\n",
    "- Vector semantics, NLP methods that aim to learn the word representations based on distributional properties of words in a large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding\n",
    "# RUNNING IN GOOGLE COLAB\n",
    "\n",
    "# Word Embedding: https://colab.research.google.com/drive/1YPvwkUNPk3N7VXMsTi1VNoyEl13HXyMa?usp=sharing\n",
    "# Training embedding gensim: https://colab.research.google.com/drive/11SL71Xf72CnFLNShbuuMiPY-LgIg4c3j?usp=sharing\n",
    "\n",
    "# import warnings #This module ignores the various types of warnings generated\n",
    "# warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# import os #This module provides a way of using operating system dependent functionality\n",
    "\n",
    "# import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
    "# process = psutil.Process(os.getpid())\n",
    "# from psutil import virtual_memory\n",
    "# mem = virtual_memory()\n",
    "\n",
    "# import time #This module is used to calculate the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec, KeyedVectors\n",
    "# pretrainedpath = 'temp/GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "# #Load W2V model. This will take some time, but it is a one time effort! \n",
    "# pre = process.memory_info().rss\n",
    "# print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "# print('-'*10)\n",
    "\n",
    "# start_time = time.time() #Start the timer\n",
    "# ttl = mem.total #Toal memory available\n",
    "\n",
    "# w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model\n",
    "# print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "# print('-'*10)\n",
    "\n",
    "# print('Finished loading Word2Vec')\n",
    "# print('-'*10)\n",
    "\n",
    "# post = process.memory_info().rss\n",
    "# print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "# print('-'*10)\n",
    "\n",
    "# print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "# print('-'*10)\n",
    "\n",
    "# print(\"Numver of words in vocablulary: \",len(w2v_model.vocab)) #Number of words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document After Pre-Processing: ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
      "------------------------------\n",
      "Average Vector of 'dog bites man'\n",
      " [ 0.587522    1.1498089  -1.7489859  -0.2872682  -1.0648674   0.3264183\n",
      "  0.15925772  0.86956954 -0.03941305 -0.05719852 -1.1595355  -0.03754699\n",
      "  0.09788382 -0.50910264 -0.09573716 -1.393181   -0.16476774 -0.9535491\n",
      "  0.89954644  2.1720686   1.0505711  -0.6770111   0.2926553  -1.2229663\n",
      " -1.3725324   2.2056735  -0.81114006 -0.8166096   1.5868374  -1.1001147\n",
      " -0.35184097  0.12588209  1.1244168  -0.84187156  0.6205936   1.1501113\n",
      "  3.149638   -0.54442066  0.3360592   0.34230545  0.2527882   2.4464004\n",
      " -0.9520517  -1.8514029  -0.41820276 -0.8024625  -0.2905644   0.24335873\n",
      "  0.67568254 -0.22497876  2.7399645  -0.29414567  0.1477855  -2.1211226\n",
      " -0.19914706  1.8373746   0.88662976 -0.61229116 -1.031511   -1.0240673\n",
      "  1.3489146  -2.9600992  -0.8370202  -2.589103    1.5814676   1.0113854\n",
      "  0.06165723 -1.9719986  -1.7209841   0.2047052   0.6108361  -0.38643467\n",
      "  0.25733945  1.3749477  -0.6103497  -0.34812152  0.5876211  -0.23538141\n",
      "  1.0789413  -3.9736195   2.9061978   0.37082872 -1.0310392   0.28841785\n",
      "  0.83239913 -1.0247461   1.5224432  -1.8909506  -1.2889     -1.1577281\n",
      " -0.09750869 -1.4510561   0.09502721  1.2903557   1.8595937   1.3561598 ]\n",
      "\n",
      "dog [ 0.25274247  1.9993004  -2.077991   -0.05172426 -1.973052   -0.9914154\n",
      " -0.91379476  0.1504401   0.8586253   0.40393114  0.00771815 -0.26777616\n",
      "  1.9737954  -0.7852856  -0.19398296 -0.2448616   0.7956637  -0.4458319\n",
      " -0.573681    3.9835021  -1.2318945  -1.1796054   0.7593868  -0.46836293\n",
      " -3.06654     1.0999677  -1.6994238  -2.0089977   0.45062187 -0.2756933\n",
      " -0.3683463   0.12000963  2.6080298  -0.5460217   2.389277    1.4182527\n",
      "  2.528384   -0.37518466  1.6271746  -0.38581273  2.0358956   3.6900785\n",
      " -1.1977513  -2.2723048   0.0782122  -1.9692171   0.13514066 -0.95442045\n",
      " -1.19424    -2.0449653   2.1446693   0.57688737 -0.09959459 -4.2137575\n",
      " -0.6928797   2.6303368   2.7579741  -2.5252466   1.0254991  -1.9347646\n",
      "  0.784384   -5.1678114  -0.77531934 -2.5365472   1.6644354   0.5410151\n",
      " -1.189408   -1.1895843  -1.8001002   2.5319152   2.7733557  -0.7658386\n",
      "  0.5598805   1.4400771  -1.8201275  -1.0073137   3.3387358   0.5453049\n",
      "  0.9963839  -3.9860837   4.2056184  -0.8719378  -0.9110155   2.8477402\n",
      " -0.4399274  -0.3572381   1.9450436  -1.577      -2.4743536  -1.1160256\n",
      " -0.5796915  -1.4415979   0.47884357  1.9918758   1.1699263   1.3125553 ]\n",
      "\n",
      "bites [-0.31782332  1.039466    0.20602697 -0.62048876 -1.880039    0.49196035\n",
      "  0.06143285 -0.16330297 -0.89926994 -1.7481537  -2.0069213   1.3786151\n",
      " -0.43823993 -1.1735275  -1.0239735  -1.9873123  -2.097607   -1.9647701\n",
      "  2.8049774  -3.2705674   4.932909   -0.5051801   0.30997843 -1.4049718\n",
      " -0.34814137  4.6400046  -0.47359163  0.41249952  1.6464217  -0.26217923\n",
      "  1.5032517   0.5357095  -1.0843124   0.77700174 -3.1479259   0.02987881\n",
      "  2.257834   -0.33247542 -0.05360803  0.38973355 -1.1151313   2.7061605\n",
      "  1.665492   -2.3279245  -0.00902647 -0.23620564  0.02887511  1.1302521\n",
      "  0.9960451  -1.5342468   3.4635596  -0.25099292  0.22376922 -2.3552315\n",
      " -1.3261505   1.6606667  -0.5410754   0.48499948 -2.4434543   0.92959535\n",
      "  1.7992399  -0.06385678 -0.6840507  -3.1247244   1.6346447   1.609806\n",
      "  1.3631198  -2.8861692  -2.6940367  -2.0642486   0.76761425 -1.2011328\n",
      " -0.5747329   4.111103   -2.2472267  -0.7983813  -0.23134175  0.432893\n",
      "  3.55152    -4.363385    4.1361995   0.6289563  -1.0057408  -0.80204403\n",
      "  3.038297   -1.0246589   1.6943691  -1.0169637  -1.3598418  -0.3863439\n",
      "  0.17340918 -1.0668007  -0.35582703  0.9920902   0.6895671   1.6112967 ]\n",
      "\n",
      "man [ 1.8276469   0.4106602  -3.374994   -0.18959153  0.6584891   1.4787099\n",
      "  1.3301351   2.6215715  -0.07759449  1.172627   -1.4794035  -1.22348\n",
      " -1.241904    0.43150502  0.93074495 -1.9473691   0.80763996 -0.450045\n",
      "  0.46734297  5.8032713  -0.5493014  -0.3462478  -0.19139938 -1.795564\n",
      " -0.7029155   0.8770485  -0.2604047  -0.8533307   2.6634688  -2.7624717\n",
      " -2.1904283  -0.2780729   1.8495331  -2.7565947   2.6204298   2.0022025\n",
      "  4.6626964  -0.92560196 -0.5653889   1.0229956  -0.16239977  0.9429619\n",
      " -3.323896   -0.9539793  -1.323794   -0.20196486 -1.0357089   0.5542445\n",
      "  2.2252426   2.904276    2.6116648  -1.2083315   0.3191819   0.20562077\n",
      "  1.4215889   1.2211204   0.44299048  0.20337364 -1.6765778  -2.0670326\n",
      "  1.46312    -3.64863    -1.0516906  -2.1060374   1.4453226   0.8833353\n",
      "  0.01125979 -1.8402423  -0.6688154   0.14644897 -1.7084615   0.8076674\n",
      "  0.7868707  -1.4263368   2.236305    0.7613305  -1.3445307  -1.6843421\n",
      " -1.3110799  -3.5713892   0.37677526  1.3554676  -1.1763616  -1.1804427\n",
      " -0.1011723  -1.6923412   0.92791665 -3.078888   -0.03250486 -1.9708147\n",
      "  0.1137563  -1.8447697   0.16206509  0.8871011   3.7192879   1.1446273 ]\n",
      "------------------------------\n",
      "Average Vector of 'man bites dog'\n",
      " [ 2.3371300e-02  1.1318434e+00 -1.9667419e+00 -6.4086390e-01\n",
      " -8.2371598e-01 -4.3177906e-01  2.7075827e-01  2.1837334e-01\n",
      " -3.5288811e-02  3.5453692e-01 -9.7565097e-01 -3.4766555e-02\n",
      "  3.5804889e-01 -2.1096520e-01  7.1270734e-01 -1.8445234e+00\n",
      "  2.6756921e-01 -6.5467900e-01  9.2796093e-01  2.0605237e+00\n",
      "  9.7211331e-01 -1.4520460e+00  6.8638515e-01 -8.0463713e-01\n",
      " -1.6067810e+00  2.0835226e+00 -4.9799654e-01 -1.1578244e+00\n",
      "  1.1688198e+00 -9.0148425e-01 -6.2999630e-01  5.0675732e-01\n",
      "  9.5519829e-01 -1.0883250e-01  6.7037481e-01  8.3818644e-01\n",
      "  3.2316363e+00 -5.8387953e-01  1.2785424e-01 -2.2079641e-01\n",
      "  1.9745414e-01  2.9917431e+00 -1.4269909e+00 -1.3384809e+00\n",
      " -2.8781763e-01 -1.3327481e+00 -2.4295776e-01 -4.2091329e-02\n",
      "  4.7322297e-01  3.4712145e-01  2.9322929e+00 -2.7164352e-01\n",
      "  2.4928648e-03 -1.7585583e+00 -5.7968378e-01  1.4915901e+00\n",
      "  1.4170493e+00 -7.6227635e-01 -6.7777687e-01 -1.7346121e+00\n",
      "  1.4760031e+00 -2.9114606e+00 -4.8324826e-01 -2.5856996e+00\n",
      "  1.7094959e+00  5.9805948e-01  5.5534393e-01 -1.6026725e+00\n",
      " -1.5314511e+00  3.6615753e-01  6.1419874e-01 -7.8812069e-01\n",
      "  8.1690079e-01  1.4488249e+00 -1.0949643e+00  1.1171404e-01\n",
      "  4.6016124e-01  2.1246830e-02  8.3943731e-01 -3.4158475e+00\n",
      "  2.5506537e+00  2.2338314e-01 -7.6826316e-01  6.6177475e-01\n",
      " -6.3722812e-02 -9.5249134e-01  1.7743460e+00 -1.8434877e+00\n",
      " -9.3641406e-01 -1.4626818e+00  5.8828849e-01 -1.8931774e+00\n",
      " -1.2629944e-01  7.1505839e-01  2.1311996e+00  1.1556960e+00]\n",
      "\n",
      "man [ 1.8766766  -0.34768388 -3.4639866  -0.8713792  -0.5940012  -0.49922925\n",
      "  1.8307434   0.9421648  -1.8591728   0.23878473 -0.10508907 -0.5231813\n",
      "  1.1039374   0.15074655 -0.442143   -1.6377698   1.3230207  -0.59228945\n",
      "  0.37539315  4.72777     0.4007545  -2.8391516   0.9191742   0.9842768\n",
      " -2.04482     1.4409885  -1.8174946  -0.07091418  0.41788343 -0.4358617\n",
      " -1.9939344   0.167285    2.4025614  -1.4473553   3.9994195   1.7176976\n",
      "  5.917016   -0.03056243  2.3277364   1.5292212  -1.2384565   3.0624404\n",
      " -1.1005502  -1.5915991  -0.45667183 -2.025956    2.3919964  -2.0350037\n",
      "  0.70501095  1.0574777   2.5875869  -0.30345404 -1.3133857  -2.3933342\n",
      "  1.0876249   0.6835833   3.8525546  -1.5995834   0.57912433 -2.8729491\n",
      "  1.711747   -3.5303364  -2.610537   -0.07090722  0.6168611   2.1995091\n",
      " -1.2991189   0.3695904  -1.2510843   1.5332292   1.8224907  -1.4235604\n",
      " -0.41334397  2.3714113  -1.3133419  -0.30882913  1.1599804  -1.2140822\n",
      " -0.85983884 -3.6970615   2.8336363   0.16421956 -2.0012743   0.27003205\n",
      " -0.30524832 -2.9878972   0.32780433 -2.8065476  -0.6348429  -1.4101695\n",
      "  0.8838198  -2.146727   -0.89261115 -0.7718329   1.3968154   0.2036863 ]\n",
      "\n",
      "bites [-0.5763172   1.0654745  -0.03799745 -1.930467   -1.0221211  -0.6914215\n",
      " -1.506141   -0.12066454 -0.09951705 -0.23582542 -2.1245701   1.4504406\n",
      " -0.37731317 -0.39465025  0.6180275  -1.2406015  -0.81059617 -2.3366194\n",
      "  2.4223473  -2.8875012   4.451759   -2.0180306   0.6919053  -1.3022393\n",
      " -0.6617817   3.693819   -0.14414905 -0.9100798   0.54495406 -0.3492536\n",
      "  1.0969124   1.2822386  -1.5664485   1.6226135  -2.6447506  -0.20320296\n",
      "  2.3611553   0.07842138 -1.0531988  -0.8453571  -1.0926888   4.3172207\n",
      "  1.1974967  -0.92012954  0.9223912  -1.0258636   0.271652   -0.08704293\n",
      " -0.04387054 -1.4724247   3.520109   -0.02068631 -0.5606921  -2.3587787\n",
      " -1.9507177   0.5256499  -0.5845698   0.4976697  -2.4481797  -0.33992797\n",
      "  2.5336246  -0.6283186  -0.6765418  -3.5845814   2.3370137   0.41261455\n",
      "  2.7323024  -2.2925596  -2.2209268  -1.7258606   1.3249223  -1.6311054\n",
      " -0.0263299   3.498993   -3.560789    0.21367139  1.2181106   0.9123076\n",
      "  3.7442415  -2.6212645   4.5258374   0.462701   -0.13925129 -0.6108005\n",
      "  0.5768783   1.1614501   2.5610242  -1.8512117  -1.349658   -1.4430528\n",
      "  1.4257071  -1.3718255  -0.45535457  0.12303302  0.8807375   0.8612951 ]\n",
      "\n",
      "dog [-1.2302455   2.6777399  -2.3982415   0.8792546  -0.85502577 -0.1046865\n",
      "  0.48767236 -0.16638023  1.8528235   1.0606515  -0.6972939  -1.031559\n",
      "  0.34752238 -0.38899192  1.9622375  -2.655199    0.29028317  0.96487176\n",
      " -0.01385766  4.3413024  -1.9361734   0.5010444   0.44807613 -2.0959487\n",
      " -2.113741    1.1157603   0.46765414 -2.4924793   2.543622   -1.9193375\n",
      " -0.9929669   0.07074845  2.029482   -0.5017558   0.6564555   1.0000645\n",
      "  1.4167373  -1.7994975  -0.8909748  -1.3462533   2.9235077   1.5955684\n",
      " -4.377919   -1.5037141  -1.3291723  -0.94642466 -3.3925216   1.9957725\n",
      "  0.75852853  1.4563113   2.6891828  -0.4907902   1.8815564  -0.5235617\n",
      " -0.8759586   3.2655373   0.98316276 -1.1849152  -0.16427529 -1.9909592\n",
      "  0.18263698 -4.5757265   1.837334   -4.10161     2.1746125  -0.81794524\n",
      "  0.23284817 -2.885048   -1.1223421   1.291104   -1.3048167   0.69030374\n",
      "  2.8903763  -1.5239297   1.5892384   0.43029988 -0.9976071   0.3655151\n",
      " -0.36609086 -3.9292161   0.29248756  0.04322889 -0.16426402  2.3260927\n",
      " -0.46279842 -1.0310271   2.4342098  -0.8727038  -0.82474136 -1.5348232\n",
      " -0.54466146 -2.1609795   0.9690674   2.793975    4.116046    2.4021068 ]\n",
      "------------------------------\n",
      "Average Vector of 'dog eats meat'\n",
      " [-0.44037962  2.609001   -1.4711336  -0.5575856  -1.5238181   1.7970337\n",
      "  0.05647969  1.2704015   0.07191402  0.5616933  -1.1977173  -0.42627466\n",
      "  1.7450209  -1.4270395  -0.90191466 -1.3282801  -1.3138363  -0.694633\n",
      "  1.7012115   0.64242554  0.6817035   0.6040221   0.02833527 -0.30315343\n",
      " -1.1411618   2.8227043  -0.31789193 -1.0328423   0.521728   -1.6472987\n",
      " -0.7488584   1.2740431   1.1167669   0.85398436 -0.9956295   1.9256172\n",
      "  1.2332166  -0.8920061   0.4477025   0.11872065  0.83144957  2.2604005\n",
      " -1.4369448  -1.077573   -0.05667556  0.9693859  -1.8964254   0.2083023\n",
      " -1.3577863  -0.33012152  2.4126234  -0.03825303  0.81320506 -0.8354141\n",
      " -0.5121463   2.7657013   1.2403277  -1.782314   -1.5316157  -0.7067601\n",
      " -0.3245783  -2.8950088  -0.38936558 -2.404454    1.5395049  -0.68764377\n",
      " -0.18316884 -1.1943358   0.76573944  2.1104386  -0.55431    -1.2024769\n",
      "  1.0208229   0.983897   -1.0135857   0.65115243  0.22453348 -0.50538033\n",
      "  1.24389    -3.874225    2.2255151   2.588211   -0.9131945   1.1695079\n",
      " -0.07111219 -1.196315    0.840578    0.12536705 -1.0578086  -0.347121\n",
      " -0.12617187 -2.506263   -0.53620195  1.5671042   1.3298488   0.19313045]\n",
      "\n",
      "dog [ 0.5841502   1.826199   -2.452287   -0.6553792  -2.4860232   2.1516442\n",
      "  0.6056237  -0.23221211  0.5961929   0.41893798 -0.75132066 -0.7609806\n",
      "  2.705065   -0.16527158 -0.1973229  -0.778546    1.621995   -0.80776227\n",
      "  0.02509803  4.9356737  -3.012389   -0.5997118   0.83000743 -0.50888014\n",
      " -2.2407842   0.01518272 -0.17223096 -0.89138925  0.45205274 -1.2073233\n",
      " -0.09456491  0.5401926   1.8851609  -0.36231852  1.0468354   2.532847\n",
      "  4.0060353  -0.9445909   3.4000964   0.9239496   1.0289952   3.9630802\n",
      " -0.9415988  -0.9477889   0.20330408 -1.190647   -1.6401246  -1.5730875\n",
      " -2.793381   -0.4955654   1.9061478   0.22876966  0.2233305  -2.4314363\n",
      " -0.6203548   1.9469088   1.4781342  -1.897094    1.4225441  -1.7407808\n",
      "  0.11490778 -5.9536176  -1.2406596  -2.1030684   1.3758745  -0.20764837\n",
      " -0.9522469  -1.2927802  -2.2562103   2.9641407   1.4816024   0.12133193\n",
      "  1.334518    0.3483986  -0.7570065  -0.7415519   2.3440826   0.7706714\n",
      "  1.232344   -4.8486834   2.7093139   0.12668955 -1.2670759   2.675593\n",
      " -0.06048152 -1.9518484   2.0603948  -0.28810433 -2.5329561  -1.3733771\n",
      " -1.1423311  -3.1440458  -1.4393688   1.5993049   2.7594118   1.3902786 ]\n",
      "\n",
      "eats [-1.3570417e+00  1.0828853e+00 -8.5662174e-01  3.4412301e-01\n",
      " -2.1097937e+00  2.0110445e+00 -1.2878094e+00  9.8084921e-01\n",
      " -1.1554983e+00  9.3838012e-01 -2.9449682e+00 -5.8532828e-01\n",
      "  7.4530405e-01 -2.5671141e+00 -3.2034454e+00 -1.8006567e+00\n",
      " -4.1374493e+00 -1.6941242e+00  2.8759689e+00 -4.0880179e+00\n",
      "  6.2775011e+00  1.3581481e+00 -2.1028899e-01  1.0899663e+00\n",
      "  6.2635064e-01  5.2446856e+00 -6.3117802e-01 -1.0129372e+00\n",
      " -1.2238996e+00 -2.2805016e+00  9.1328740e-01  1.5006480e+00\n",
      " -9.9875581e-01  3.5838494e+00 -3.9268892e+00  1.8244939e+00\n",
      "  6.9051588e-01 -1.1444525e+00 -7.4136108e-01  4.4139838e-01\n",
      "  1.3169463e+00 -8.5439324e-01  1.9049237e+00 -1.5549164e+00\n",
      " -3.9405364e-01  1.7028115e+00 -3.5459690e+00  1.0258704e+00\n",
      " -2.0091753e+00 -2.0503433e+00  5.7964606e+00  6.6993332e-01\n",
      "  1.4714385e+00  8.2684147e-01  4.7747588e-01  2.6338083e-01\n",
      " -1.1441407e+00 -1.0321405e+00 -3.5956552e+00  7.9377717e-01\n",
      " -2.8851919e+00  1.0437722e+00  2.6561320e-03 -5.0994453e+00\n",
      "  1.9408833e+00 -9.2360187e-01  1.2810456e+00 -2.7416962e-01\n",
      "  4.0063229e+00 -5.6680989e-01  5.4788363e-01 -1.7113631e+00\n",
      " -7.2110188e-01  4.5566797e+00 -3.7699192e+00 -7.0649907e-02\n",
      "  9.6863031e-01 -9.5010245e-01  3.2997422e+00 -4.4692020e+00\n",
      "  3.5284443e+00  4.5871310e+00 -4.2124146e-01 -4.8004687e-01\n",
      "  1.6426971e+00  6.1511308e-01  5.6593847e-01  1.8153633e+00\n",
      "  5.0483698e-01  1.7893809e-01  1.2488732e+00 -4.9692273e-02\n",
      " -1.0118636e+00  1.8153528e+00 -1.0284847e+00 -1.2969002e+00]\n",
      "\n",
      "meat [-0.5482474   4.9179187  -1.1044918  -1.3615006   0.02436209  1.2284124\n",
      "  0.8516247   3.0625675   0.7750474   0.32776177  0.10313708  0.06748497\n",
      "  1.7846934  -1.548733    0.69502425 -1.4056376  -1.4260547   0.41798738\n",
      "  2.2025673   1.0796208  -1.2200017   1.0536299  -0.5347126  -1.4905465\n",
      " -1.8090518   3.2082443  -0.15026677 -1.1942005   2.337031   -1.4540709\n",
      " -3.0652976   1.7812886   2.4638958  -0.6595777  -0.10683459  1.4195106\n",
      " -0.99690133 -0.58697474 -1.3156278  -1.009186    0.14840722  3.6725142\n",
      " -5.2741594  -0.7300133   0.02072287  2.3959932  -0.5031829   1.1721239\n",
      "  0.72919685  1.5555441  -0.46473855 -1.0134621   0.7448461  -0.90164745\n",
      " -1.3935599   6.0868144   3.3869898  -2.4177077  -2.4217362  -1.1732765\n",
      "  1.7965493  -3.7751808   0.06990677 -0.01084828  1.3017569  -0.931681\n",
      " -0.8783052  -2.0160575   0.54710567  3.9339852  -3.6924162  -2.0173995\n",
      "  2.4490526  -1.9533875   1.4861684   2.765659   -2.6391125  -1.33671\n",
      " -0.80041623 -2.30479     0.43878675  3.0508127  -1.0512662   1.3129776\n",
      " -1.7955521  -2.25221    -0.10459913 -1.1511579  -1.1453068   0.15307593\n",
      " -0.4850577  -4.325051    0.8426265   1.2866548   2.258619    0.48601294]\n",
      "------------------------------\n",
      "Average Vector of 'man eats food'\n",
      " [ 0.44236562  1.2266035  -2.471268   -1.0272282   0.12377938  1.2504691\n",
      "  1.752139    1.7411829  -0.4186965   0.92732716 -1.6711885  -0.5258811\n",
      "  0.31537816 -1.425416    0.04559986 -1.6090702  -0.83947164 -2.0291321\n",
      "  1.907744    0.8391244   0.45793876 -0.28334507  0.9681229   0.26006117\n",
      " -0.69782346  2.3975928  -0.9744835  -0.74852777  1.3961667  -1.4803079\n",
      " -0.64796716  0.30300233  1.0654316   0.05148005  0.15935339  1.2149802\n",
      "  1.0984033  -0.1534778   0.9463577   1.3296853  -0.7985998   0.9802489\n",
      " -1.3091758  -1.2526339   0.13951033  0.11918541 -0.24406676 -0.6196454\n",
      " -0.46598968 -0.13168931  2.4972131   0.5649746   0.565654   -0.3303454\n",
      "  0.4847879   1.3551105   1.6029266  -1.7816391  -1.8098202  -1.5117365\n",
      " -0.13120179 -3.6811      0.16754581 -2.3100958   0.39367545  0.04822301\n",
      "  0.02528095  0.48746404  0.20792674  2.164746   -0.3786067  -0.5559729\n",
      "  0.15455012  1.784109   -1.8140599   0.41032764  0.0243394  -1.1828332\n",
      "  1.3390552  -3.142883    2.8877668   1.9884691  -0.8283357  -0.014908\n",
      " -0.30270562 -1.0364925   1.0901874   0.20984244 -1.1820072  -0.34178007\n",
      "  1.3546219  -1.8177758  -1.0850339  -0.17058992  0.87786704  0.31901374]\n",
      "\n",
      "man [ 1.5027351  -0.48657972 -3.743112   -1.2674911  -0.74680793  1.7661874\n",
      "  2.9264812   0.45062009 -0.84592324  0.638269   -0.8594957  -0.79715097\n",
      "  1.0330497  -0.01903754  0.05249856 -2.0316067   1.6913658  -1.9680347\n",
      "  2.1057637   6.0542107  -1.9689822  -2.4422135   0.84545225  0.20914918\n",
      " -1.6354195   0.1637879   0.15208709  0.5710604   0.43214366 -1.1170179\n",
      " -1.3771539   0.44643688  1.3663508  -1.4986404   2.9038937   3.019674\n",
      "  7.1578956   0.11579812  4.1195016   2.6146994  -2.2911887   2.841639\n",
      " -1.2570915  -0.6314633  -0.45517045 -2.0509624   0.8679409  -2.9046206\n",
      " -0.6109835   1.5642759   1.4593287  -0.49088496 -0.64512575 -1.5147328\n",
      "  1.3473207  -0.2622903   2.2510967  -0.1541653   0.40272617 -1.9093946\n",
      "  1.8254406  -5.1684422  -2.7908409   0.7806679   0.35353747  0.9927618\n",
      " -0.88507646  0.67525303 -2.166862    2.2242558   0.47327513 -1.2358947\n",
      " -0.06696784  1.5701885  -1.501277   -0.7549242   0.1896931  -0.8901162\n",
      " -0.7689618  -3.6346397   2.812205    0.8060961  -2.5900538  -0.3968806\n",
      "  0.75974524 -2.83319     0.4615407  -0.91887784 -0.994221   -2.0083382\n",
      "  0.42543232 -2.996358   -2.2122202  -0.41311893  2.003055    0.57297355]\n",
      "\n",
      "eats [-0.39621451 -0.12156114 -1.7837065  -0.2357378  -2.4344432   1.1799217\n",
      " -2.5180583   0.7803121  -1.2543955   2.1563506  -1.7195576  -1.5740528\n",
      " -0.465456   -2.503587   -1.0387955  -2.2346716  -3.3778453  -2.3079233\n",
      "  1.5932217  -3.7967644   3.7427344   0.49515957  1.525979    0.79255986\n",
      "  0.9441325   4.884795   -0.81902146 -2.4853926  -0.35785952  0.5968965\n",
      "  2.061945    1.399198   -0.38824517  3.680757   -3.4114196   0.83015907\n",
      " -1.1374661  -0.55586934 -1.765626    0.5962699  -0.07768139 -0.25614142\n",
      "  1.3867438   0.5223505  -0.03151423  1.3807799  -2.165023   -1.191783\n",
      " -1.7023304  -3.8536718   5.5320425   0.9486916   2.2586021   0.9437685\n",
      "  0.7700845  -0.42024308  0.07717609 -2.5394664  -2.5886493   0.9331712\n",
      " -2.202001   -0.4264279   2.3927839  -4.4562154   1.5159985  -0.8674011\n",
      "  0.20902175  0.86177534  3.0688095  -0.5721316   0.48399746 -1.3507001\n",
      " -0.17716813  4.4724298  -4.2225337   2.2338593   1.6861417  -2.1109273\n",
      "  3.3975408  -4.11477     3.3662841   4.8124065   0.8942727  -0.70257884\n",
      "  0.22794208  1.8016557   2.1238122   2.066041   -0.76263154 -0.6296528\n",
      "  2.215444   -1.0763849  -1.6377968  -0.30422103 -1.2981335  -1.479615  ]\n",
      "\n",
      "food [ 0.22057617  4.2879515  -1.8869851  -1.5784557   3.5525892   0.80529785\n",
      "  4.847994    3.9926167   0.84422916 -0.01263812 -2.4345121   0.7935606\n",
      "  0.3785408  -1.7536232   1.1230965  -0.5609324  -0.8319354  -1.811438\n",
      "  2.024247    0.25992677 -0.39993602  1.0970187   0.5329372  -0.22152549\n",
      " -1.4021834   2.1441958  -2.256516   -0.33125114  4.114216   -3.9208026\n",
      " -2.6286926  -0.936628    2.2181892  -2.0276763   0.9855861  -0.2048921\n",
      " -2.7252197  -0.0203622   0.48519754  0.7780867  -0.02692939  0.3552492\n",
      " -4.05718    -3.648789    0.9052157   1.0277388   0.5648819   2.2374673\n",
      "  0.9153449   1.8943279   0.5002685   1.2371173   0.08348548 -0.42007187\n",
      " -0.6630415   4.7478647   2.480507   -2.651286   -3.2435377  -3.5589864\n",
      " -0.0170449  -5.4484296   0.90069443 -3.2547398  -0.6885096   0.01930836\n",
      "  0.7518976  -0.07463622 -0.2781673   4.842114   -2.0930927   0.9186762\n",
      "  0.7077863  -0.6902916   0.28163075 -0.24795222 -1.8028166  -0.54745615\n",
      "  1.3885866  -1.6792395   2.4848113   0.34690467 -0.789226    1.0547354\n",
      " -1.8958042  -2.0779433   0.6852093  -0.5176358  -1.7891691   1.6126508\n",
      "  1.4229892  -1.3805848   0.5949153   0.20557022  1.9286795   1.8636826 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Document After Pre-Processing:\",processed_docs)\n",
    "\n",
    "# Iterate over each document and initiate an nlp instance.\n",
    "for doc in processed_docs:\n",
    "    doc_nlp = nlp(doc) #creating a spacy \"Doc\" object which is a container for accessing linguistic annotations. \n",
    "    \n",
    "    print(\"-\"*30)\n",
    "    print(\"Average Vector of '{}'\\n\".format(doc),doc_nlp.vector)#this gives the average vector of each document\n",
    "    for token in doc_nlp:\n",
    "        print()\n",
    "        print(token.text,token.vector)#this gives the text of each word in the doc and their respective vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to handle OOV problem for word embeddings:\n",
    "1. Create vectors that are initialized randomly, where each component between -0.25 to +0.25.\n",
    "2. Subword, morphological properties (prefixes, suffixes, word endings, etc), or by using character representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Representations Beyond Words and Characters\n",
    "\n",
    "Word2vec learned representations for words, and we aggregated them to form text representations. fastText learned representations for character n-grams, which were aggregated to form word representations and then text representations.\n",
    "\n",
    "Both approaches do not take the context of words into account. For example, the sentences \"dog bites man\" and \"man bites dog\", both receive the same representation.\n",
    "\n",
    "Another approaches, Doc2vec, allows us to directly learn the representations for texts of arbitrary lengths (phrases, sentences, paragraphs, and documents) by taking the context of words in the text into account.\n",
    "\n",
    "Doc2vec learns a \"paragraph vector\" that learns a representation for the full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yasir Abdur\n",
      "[nltk_data]     Rohman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['dog', 'bites', 'man'], tags=['0']),\n",
       " TaggedDocument(words=['man', 'bites', 'dog'], tags=['1']),\n",
       " TaggedDocument(words=['dog', 'eats', 'meat'], tags=['2']),\n",
       " TaggedDocument(words=['man', 'eats', 'food'], tags=['3'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"dog bites man\",\n",
    "        \"man bites dog\",\n",
    "        \"dog eats meat\",\n",
    "        \"man eats food\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(word.lower()), tags=[str(i)]) for i, word in enumerate(data)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00381694 -0.01083953  0.02366928 -0.01721474 -0.01125923 -0.02391598\n",
      "  0.00957225 -0.01906247  0.01022941 -0.02121767 -0.02251468 -0.01903932\n",
      " -0.0064447   0.02034412  0.02025429 -0.02116948  0.02440305  0.01085662\n",
      " -0.01993765 -0.01649801]\n"
     ]
    }
   ],
   "source": [
    "#dbow\n",
    "model_dbow = Doc2Vec(tagged_data,vector_size=20, min_count=1, epochs=2,dm=0)\n",
    "print(model_dbow.infer_vector(['man','eats','food']))#feature vector of man eats food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meat', 0.32583820819854736),\n",
       " ('eats', 0.28601741790771484),\n",
       " ('food', 0.14761802554130554),\n",
       " ('bites', -0.20909522473812103),\n",
       " ('dog', -0.24991711974143982)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.wv.most_similar(\"man\",topn=5)#top 5 most simlar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2860174"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.wv.n_similarity([\"eats\"],[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Vector of man eats food\n",
      "  [ 0.00381702 -0.01083956  0.02366922 -0.01721474 -0.01125922 -0.02391591\n",
      "  0.00957238 -0.01906254  0.01022946 -0.02121754 -0.02251468 -0.01903929\n",
      " -0.00644476  0.02034397  0.02025425 -0.02116957  0.0244031   0.0108567\n",
      " -0.01993772 -0.01649793]\n",
      "Most similar words to man in our corpus\n",
      " [('meat', 0.32583820819854736), ('eats', 0.28601741790771484), ('food', 0.14761802554130554), ('bites', -0.20909522473812103), ('dog', -0.24991711974143982)]\n",
      "Similarity between man and dog:  -0.24991713\n"
     ]
    }
   ],
   "source": [
    "#dm\n",
    "model_dm = Doc2Vec(tagged_data, min_count=1, vector_size=20, epochs=2,dm=1)\n",
    "\n",
    "print(\"Inference Vector of man eats food\\n \",model_dm.infer_vector(['man','eats','food']))\n",
    "\n",
    "print(\"Most similar words to man in our corpus\\n\",model_dm.wv.most_similar(\"man\",topn=5))\n",
    "print(\"Similarity between man and dog: \",model_dm.wv.n_similarity([\"dog\"],[\"man\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOV\n",
    "# model_dm.wv.n_similarity(['covid'],['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Text Representations\n",
    "\n",
    "Words can mean different things in different context. For example\n",
    "- “I went to a **bank** to withdraw money” and\n",
    "- “I sat by the river **bank** and pondered about text representations”\n",
    "\n",
    "Contextual word representations, which addresses this issue. It uses \"language modeling\" which is the task of predictiong the next likely word in a sequence of words.\n",
    "- Transformers, BERT, ELMo, etc\n",
    "\n",
    "Important aspects to keep in mind while using them in our project:\n",
    "1. All text representations are inherently biases based on what they saw in training data.\n",
    "    - Example: An embedding model trained heavily on technology data is likely to identify Apple as being closer to Microsoft/Facebook that to an orange.\n",
    "2. Pre-trained embeddings are generally large-sized files (several GBs), which may pose problems in certain deployment scenarios.\n",
    "3. Modeling language for a real-world application is more that capturing the information via word and sentence embeddings.\n",
    "    - Example: the task of sarcasm detection requires nuances that are not yet captured well by embedding techniques.\n",
    "4. A practitioner needs to exercise caution and consider practical issues such as return on investment from the effort, business needs, and infrastructural constraints before trying to use them in production-grade applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "# Visualizing Embeddings using TSNE:\n",
    "# https://colab.research.google.com/drive/1HJ60cZe2DZdorHVHcMOHDWSqLZQuRURD?usp=sharing\n",
    "\n",
    "# Visualizing Embeddings using Tensorboard\n",
    "# https://colab.research.google.com/drive/1s2GsIztRNuSMoBHAaN1vf9ChjGCwvdZK?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handcrafted Feature Representation\n",
    "\n",
    "In many cases, we do have some domain-specific knowledge about the given NLP problem, which we would like to incorporate into the model we're building.\n",
    "\n",
    "Clearly, custom feature engineering is much more difficult to formulate compared to other feature engineering schemes we’ve seen so far. It’s for this reason that vectorization approaches are more accessible to get started with, especially when we don’t have enough understanding of the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
