{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Training Data\n",
    "\n",
    "Task: \n",
    "- Design a classifier for segregating customer compaints for e-commerce company.\n",
    "- The classifier need to automatically route customer compaints emails into a set of categories: billing, delivery, and others.\n",
    "\n",
    "Problem:\n",
    "- Database doesn't exist\n",
    "\n",
    "Solution:\n",
    "- Create annotated dataset where customer complaints are mapped to ther set of categories.\n",
    "- We can investigate the dataset perhaps there are some patterns in each category.\n",
    "- Use tool like Snorkel, which is useful for deploying weak supervision for various learning taks, such as classfication.\n",
    "- Crowdsourcing can be seen as an option to label the data, likes Amazon Mechanical Turk and Figure Eight.\n",
    "\n",
    "A popular example of using the wisdom of crowds to create a classification dataset is the CAPTCHA test, which Google users to ask if a set of images contain a given object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Less Training Data: Active Learning and Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning\n",
    "Sometimes turn out that \n",
    "- the amount of data is too small to build a good classification model or\n",
    "- most of the requests we collected belonged to billing and very few belonged to other categories, resulting in a highly imbalanced dataset\n",
    "- and we cannot asking the agents to spend many hours doing manual annotation\n",
    "\n",
    "The approach that can be used is active learning, which is primarily about identifying which data points are more crucial to be used as training data.\n",
    "\n",
    "Active learning steps:\n",
    "1. Train the classifier with the available amount of data.\n",
    "2. Start using the classifier to make predictions on new data.\n",
    "3. For the data points where the classifier is very unsure of its predictions, send them to human annotators for their correct classification.\n",
    "4. Include these data points in the existing training data and retrain the model.\n",
    "\n",
    "Prodigy, tools that have active learning solutions implemented for text classification and support the efficient usage of active learning to create annotated data and text classification models quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adaptation (Transfer Learning)\n",
    "\n",
    "Imagine a scenarion for our customer complaint classifier where we have a lot of historical data for a range of products.\n",
    "\n",
    "We asked to tune it to work on a set of NEWER PRODUCTS.\n",
    "We have problems, because typical text classification approaches rely on the vocabulary of the training data. Hence, they’re inherently biased toward the kind of language seen in the training data.\n",
    "It’s also not realistic to train a new model from scratch on each product or product suite, as we’ll again run into the problem of insufficient training data.\n",
    "\n",
    "These problems can be solved using Transfer Learning.\n",
    "We “transfer” what we learned from one domain (source) with large amounts of data to another domain (target) with less labeled data but large amounts of unlabeled data.\n",
    "\n",
    "Transfer learning steps:\n",
    "1. Start with a large, pre-trained language model trained on a large dataset of the source domain (e.g., Wikipedia data).\n",
    "2. Fine-tune this model using the target language’s unlabeled data.\n",
    "3. Train a classifier on the labeled target domain data by extracting feature representations from the fine-tuned language model from Step 2.\n",
    "\n",
    "**ULMFit** is another popular domain adaptation approach for text classification. In research experiments, it was shown that this approach matches the performance of training from scratch with 10 to 20 times more training examples and only 100 labeled examples in text classification tasks. When unlabeled data was used to fine-tune the pre-trained language model, it matched the performance of using 50 to 100 times more labeled examples when trained from scratch, on the same text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Corporate Ticketing\n",
    "\n",
    "We are asked to build a ticketing system for our organization that will track all the tickets or issues people face in the organization and route them to either internal or external agents.\n",
    "\n",
    "Problems:\n",
    "- Now let’s say our company has recently hired a medical counsel and partnered with a hospital. So our system should also be able to pinpoint any medical-related issue and route it to the relevant people and teams. But while we have some past tickets, none of them are labeled as health related. In the absence of these labels, how will we go about building such a health issue–related classification system?\n",
    "\n",
    "Options:\n",
    "1. **Use existing APIs or libraries**: Start with a public API or library and map its classes to what’s relevant to us. Example: Google APIs.\n",
    "2. **Use public datasets**: adopt public datasets\n",
    "3. **Utilize weak supervision**: We have a history of past tickets, but they’re not labeled. So, we can consider bootstrapping a dataset.\n",
    "    - Example: consider having a rule: “If the past ticket contains words like fever, diarrhea, headache, or nausea, put them in the medical counsel category.”\n",
    "4. **Active learning**: use tools like Prodigy\n",
    "5. **Learning from implicit and explicit feedback**:\n",
    "    - Explicit feedback could be when the medical counsel or hospital says explicitly that the ticket was not relevant. \n",
    "    - Implicit feedback could be extracted from other dependent variables like ticket response times and ticket response rates. All of these could be factored in to improve our model using active learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Advice\n",
    "- Establish strong baselines: always good to start with simple approaches and try to establish strong baselines first. 3 reasons:\n",
    "    1. It helps us get a better understanding of the problem statement and key challenges.\n",
    "    2. Building a quick MVP helps us get initial feedback from end users and stakeholders.\n",
    "    3. A state-of-the-art research model may give us only a minor improvement compared to the baseline, but it might come with a huge amount of technical debt.\n",
    "    \n",
    "- Balance training data: imbalanced can adversely impact the learning algorithm and result in a biased classifier.\n",
    "- Combine models and humans in the loop\n",
    "- Make it work, make it better: It is always good to build a model quickly, use it to build a system, then start improvement iterations\n",
    "- Use the wisdom of many (Ensembling): Every text classification algorithm has its own strengths and weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
