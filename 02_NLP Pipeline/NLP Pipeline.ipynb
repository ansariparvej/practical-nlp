{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "How to get the data?\n",
    "1. Use a public dataset\n",
    "2. Scrape data\n",
    "3. Product intervention: AI team work with the product team to collect more data by developing better instrumentation.\n",
    "4. Data augmentation\n",
    "    - Synonym replacement\n",
    "    - Back translation\n",
    "    - TF-IDF-based word replacement\n",
    "    - Bigram flipping\n",
    "    - Replacing entities\n",
    "    - Adding noises to data\n",
    "    - Advanced techniques: Snorkel, Easy Data Augmentation (EDA), Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML parsing and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the module/method used to get the current time?\n",
      "Best answer: \n",
      " Use:\n",
      ">>> import datetime\n",
      ">>> datetime.datetime.now()\n",
      "datetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now())\n",
      "2009-01-06 15:08:24.789150\n",
      "\n",
      "And just the time:\n",
      ">>> datetime.datetime.now().time()\n",
      "datetime.time(15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now().time())\n",
      "15:08:24.789150\n",
      "\n",
      "See the documentation for more information.\n",
      "To save typing, you can import the datetime object from the datetime module:\n",
      ">>> from datetime import datetime\n",
      "\n",
      "Then remove the leading datetime. from all of the above.\n"
     ]
    }
   ],
   "source": [
    "myurl = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\"\n",
    "\n",
    "html = urlopen(myurl).read()\n",
    "soupified = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "question = soupified.find(\"div\", {\"class\": \"question\"})\n",
    "questiontext = question.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
    "print(\"Question: \\n\", questiontext.get_text().strip())\n",
    "\n",
    "answer = soupified.find(\"div\", {\"class\": \"answer\"})\n",
    "answertext = answer.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
    "print(\"Best answer: \\n\", answertext.get_text().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'I love \\xf0\\x9f\\x8d\\x95!  Shall we book a \\xf0\\x9f\\x9a\\x97 to gizza?'\n"
     ]
    }
   ],
   "source": [
    "text = 'I love üçï!  Shall we book a üöó to gizza?'\n",
    "Text = text.encode(\"utf-8\")\n",
    "print(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"code\": \"401\",\n",
      "        \"message\": \"Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Need azure upgraded account\n",
    "# Tutorial: https://docs.microsoft.com/en-us/azure/cognitive-services/bing-spell-check/quickstarts/python\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "api_key = \"<ENTER-KEY-HERE>\"\n",
    "example_text = \"Hollo, wrld\"\n",
    "endpoint = \"https://api.cognitive.microsoft.com/bing/v7.0/SpellCheck\"\n",
    "\n",
    "data = {'text': example_text}\n",
    "params = {\n",
    "    'mkt':'en-us',\n",
    "    'mode':'proof'\n",
    "    }\n",
    "headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    'Ocp-Apim-Subscription-Key': api_key,\n",
    "    }\n",
    "response = requests.post(endpoint, headers=headers, params=params, data=data)\n",
    "json_response = response.json()\n",
    "print(json.dumps(json_response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System-specific error correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the nineteenth century the only Kind of linguistics considered\n",
      "seriously was this comparative and historical study of words in languages\n",
      "known or believed to be cognate‚Äîsay the Semitic languages, or the Indo-\n",
      "European languages. It is significant that the Germans who really made\n",
      "the subject what it was, used the term Indo-germanisch. Those who know\n",
      "the popular works of Otto Jespersen will remember how fitmly he\n",
      "declares that linguistic science is historical. And those who have noticed\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "# need to follow this steps to use tesseract\n",
    "# https://stackoverflow.com/questions/50951955/pytesseract-tesseractnotfound-error-tesseract-is-not-installed-or-its-not-i\n",
    "pytesseract.pytesseract.tesseract_cmd = r'D:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "filename = \"scanned_document.png\"\n",
    "text = image_to_string(Image.open(filename))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "- Preliminaries:\n",
    "    - Sentence segmentation, word tokenization\n",
    "- Frequent steps:\n",
    "    - stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing.\n",
    "- Other steps:\n",
    "    - normalization, language detection, code mixing, transliteration, etc.\n",
    "- Advanced processing:\n",
    "    - POS tagging, parsing, coreference resolution, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.',\n",
       " 'If we were asked to build such an application, think about how we would approach doing so at our organization.',\n",
       " 'We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.',\n",
       " 'Since language processing is involved, we would also list all the forms of text processing needed at each step.',\n",
       " 'This step-by-step processing of text is known as pipeline.',\n",
       " 'It is the series of steps involved in building any NLP model.',\n",
       " 'These steps are common in every NLP project, so it makes sense to study them in this chapter.',\n",
       " 'Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.',\n",
       " 'Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.',\n",
       " 'In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines about when and how to use which step.',\n",
       " 'In later chapters, we‚Äôll discuss specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence segmentation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "mytext = \"In the previous chapter, we saw examples of some common NLP \\\n",
    "applications that we might encounter in everyday life. If we were asked to \\\n",
    "build such an application, think about how we would approach doing so at our \\\n",
    "organization. We would normally walk through the requirements and break the \\\n",
    "problem down into several sub-problems, then try to develop a step-by-step \\\n",
    "procedure to solve them. Since language processing is involved, we would also \\\n",
    "list all the forms of text processing needed at each step. This step-by-step \\\n",
    "processing of text is known as pipeline. It is the series of steps involved in \\\n",
    "building any NLP model. These steps are common in every NLP project, so it \\\n",
    "makes sense to study them in this chapter. Understanding some common procedures \\\n",
    "in any NLP pipeline will enable us to get started on any NLP problem encountered \\\n",
    "in the workplace. Laying out and developing a text-processing pipeline is seen \\\n",
    "as a starting point for any NLP application development process. In this \\\n",
    "chapter, we will learn about the various steps involved and how they play \\\n",
    "important roles in solving the NLP problem and we‚Äôll see a few guidelines \\\n",
    "about when and how to use which step. In later chapters, we‚Äôll discuss \\\n",
    "specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).\"\n",
    "\n",
    "my_sentences = sent_tokenize(mytext)\n",
    "my_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.\n",
      "['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'examples', 'of', 'some', 'common', 'NLP', 'applications', 'that', 'we', 'might', 'encounter', 'in', 'everyday', 'life', '.']\n",
      "If we were asked to build such an application, think about how we would approach doing so at our organization.\n",
      "['If', 'we', 'were', 'asked', 'to', 'build', 'such', 'an', 'application', ',', 'think', 'about', 'how', 'we', 'would', 'approach', 'doing', 'so', 'at', 'our', 'organization', '.']\n",
      "We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.\n",
      "['We', 'would', 'normally', 'walk', 'through', 'the', 'requirements', 'and', 'break', 'the', 'problem', 'down', 'into', 'several', 'sub-problems', ',', 'then', 'try', 'to', 'develop', 'a', 'step-by-step', 'procedure', 'to', 'solve', 'them', '.']\n",
      "Since language processing is involved, we would also list all the forms of text processing needed at each step.\n",
      "['Since', 'language', 'processing', 'is', 'involved', ',', 'we', 'would', 'also', 'list', 'all', 'the', 'forms', 'of', 'text', 'processing', 'needed', 'at', 'each', 'step', '.']\n",
      "This step-by-step processing of text is known as pipeline.\n",
      "['This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.']\n",
      "It is the series of steps involved in building any NLP model.\n",
      "['It', 'is', 'the', 'series', 'of', 'steps', 'involved', 'in', 'building', 'any', 'NLP', 'model', '.']\n",
      "These steps are common in every NLP project, so it makes sense to study them in this chapter.\n",
      "['These', 'steps', 'are', 'common', 'in', 'every', 'NLP', 'project', ',', 'so', 'it', 'makes', 'sense', 'to', 'study', 'them', 'in', 'this', 'chapter', '.']\n",
      "Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.\n",
      "['Understanding', 'some', 'common', 'procedures', 'in', 'any', 'NLP', 'pipeline', 'will', 'enable', 'us', 'to', 'get', 'started', 'on', 'any', 'NLP', 'problem', 'encountered', 'in', 'the', 'workplace', '.']\n",
      "Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.\n",
      "['Laying', 'out', 'and', 'developing', 'a', 'text-processing', 'pipeline', 'is', 'seen', 'as', 'a', 'starting', 'point', 'for', 'any', 'NLP', 'application', 'development', 'process', '.']\n",
      "In this chapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we‚Äôll see a few guidelines about when and how to use which step.\n",
      "['In', 'this', 'chapter', ',', 'we', 'will', 'learn', 'about', 'the', 'various', 'steps', 'involved', 'and', 'how', 'they', 'play', 'important', 'roles', 'in', 'solving', 'the', 'NLP', 'problem', 'and', 'we', '‚Äô', 'll', 'see', 'a', 'few', 'guidelines', 'about', 'when', 'and', 'how', 'to', 'use', 'which', 'step', '.']\n",
      "In later chapters, we‚Äôll discuss specific pipelines for various NLP tasks (e.g., Chapters 4‚Äì7).\n",
      "['In', 'later', 'chapters', ',', 'we', '‚Äô', 'll', 'discuss', 'specific', 'pipelines', 'for', 'various', 'NLP', 'tasks', '(', 'e.g.', ',', 'Chapters', '4‚Äì7', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization\n",
    "for sentence in my_sentences:\n",
    "    print(sentence)\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent steps\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token.lower() for token in tokens if token not in mystopwords and \n",
    "                not token.isdigit() and token not in punctuation]\n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in',\n",
       "  'previous',\n",
       "  'chapter',\n",
       "  'saw',\n",
       "  'examples',\n",
       "  'common',\n",
       "  'nlp',\n",
       "  'applications',\n",
       "  'might',\n",
       "  'encounter',\n",
       "  'everyday',\n",
       "  'life',\n",
       "  'if',\n",
       "  'asked',\n",
       "  'build',\n",
       "  'application',\n",
       "  'think',\n",
       "  'would',\n",
       "  'approach',\n",
       "  'organization',\n",
       "  'we',\n",
       "  'would',\n",
       "  'normally',\n",
       "  'walk',\n",
       "  'requirements',\n",
       "  'break',\n",
       "  'problem',\n",
       "  'several',\n",
       "  'sub-problems',\n",
       "  'try',\n",
       "  'develop',\n",
       "  'step-by-step',\n",
       "  'procedure',\n",
       "  'solve',\n",
       "  'since',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'involved',\n",
       "  'would',\n",
       "  'also',\n",
       "  'list',\n",
       "  'forms',\n",
       "  'text',\n",
       "  'processing',\n",
       "  'needed',\n",
       "  'step',\n",
       "  'this',\n",
       "  'step-by-step',\n",
       "  'processing',\n",
       "  'text',\n",
       "  'known',\n",
       "  'pipeline',\n",
       "  'it',\n",
       "  'series',\n",
       "  'steps',\n",
       "  'involved',\n",
       "  'building',\n",
       "  'nlp',\n",
       "  'model',\n",
       "  'these',\n",
       "  'steps',\n",
       "  'common',\n",
       "  'every',\n",
       "  'nlp',\n",
       "  'project',\n",
       "  'makes',\n",
       "  'sense',\n",
       "  'study',\n",
       "  'chapter',\n",
       "  'understanding',\n",
       "  'common',\n",
       "  'procedures',\n",
       "  'nlp',\n",
       "  'pipeline',\n",
       "  'enable',\n",
       "  'us',\n",
       "  'get',\n",
       "  'started',\n",
       "  'nlp',\n",
       "  'problem',\n",
       "  'encountered',\n",
       "  'workplace',\n",
       "  'laying',\n",
       "  'developing',\n",
       "  'text-processing',\n",
       "  'pipeline',\n",
       "  'seen',\n",
       "  'starting',\n",
       "  'point',\n",
       "  'nlp',\n",
       "  'application',\n",
       "  'development',\n",
       "  'process',\n",
       "  'in',\n",
       "  'chapter',\n",
       "  'learn',\n",
       "  'various',\n",
       "  'steps',\n",
       "  'involved',\n",
       "  'play',\n",
       "  'important',\n",
       "  'roles',\n",
       "  'solving',\n",
       "  'nlp',\n",
       "  'problem',\n",
       "  '‚Äô',\n",
       "  'see',\n",
       "  'guidelines',\n",
       "  'use',\n",
       "  'step',\n",
       "  'in',\n",
       "  'later',\n",
       "  'chapters',\n",
       "  '‚Äô',\n",
       "  'discuss',\n",
       "  'specific',\n",
       "  'pipelines',\n",
       "  'various',\n",
       "  'nlp',\n",
       "  'tasks',\n",
       "  'e.g.',\n",
       "  'chapters',\n",
       "  '4‚Äì7']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_corpus([mytext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Yasir Abdur\n",
      "[nltk_data]     Rohman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car revolut\n",
      "good\n",
      "better well\n"
     ]
    }
   ],
   "source": [
    "# stemming and lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "    \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "word1, word2 = \"cars\", \"revolution\"\n",
    "print(stemmer.stem(word1), stemmer.stem(word2))\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) #a is for adjective\n",
    "\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "token = sp(u'better')\n",
    "for word in token:\n",
    "    print(word.text,  word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Charles PROPN Xxxxx True False\n",
      "Spencer Spencer PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "was be AUX xxx True True\n",
      "born bear VERB xxxx True False\n",
      "on on ADP xx True True\n",
      "16 16 NUM dd False False\n",
      "April April PROPN Xxxxx True False\n",
      "1889 1889 NUM dddd False False\n",
      "toHannah toHannah PROPN xxXxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "( ( PUNCT ( False False\n",
      "born bear VERB xxxx True False\n",
      "Hannah Hannah PROPN Xxxxx True False\n",
      "Harriet Harriet PROPN Xxxxx True False\n",
      "Pedlingham Pedlingham PROPN Xxxxx True False\n",
      "Hill Hill PROPN Xxxx True False\n",
      ") ) PUNCT ) False False\n",
      "and and CCONJ xxx True True\n",
      "Charles Charles PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "Sr Sr PROPN Xx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Charles Spencer Chaplin was born on 16 April 1889 toHannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- Classical NLP/ML Pipeline\n",
    "- DL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "- Start with Simple Heuristics\n",
    "- Building Your Model\n",
    "- Building The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Depends on two factors:\n",
    "1. Using the right metric for evaluation\n",
    "2. Following the right evaluation process\n",
    "\n",
    "Two types of evaluation:\n",
    "1. Intrinsic: focuses on intermediary objectives, ex: precision and recall\n",
    "2. Extrinsic: focuses on evaluating performance on the final objective, ex: the amount of time users spent on a spam email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Modeling Phases\n",
    "- Deployment\n",
    "- Monitoring\n",
    "- Updating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
