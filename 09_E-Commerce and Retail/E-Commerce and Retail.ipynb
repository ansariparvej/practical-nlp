{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Aplications in E-Commerce\n",
    "\n",
    "![alt text](https://learning.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0901.png)\n",
    "\n",
    "- E-Commerce Catalog, a product catalog is a database of the products that the enterprise deals or a user can purchase.\n",
    "- Review Analysis, analysis from user reviews for every product.\n",
    "- Product Search, search engine for products.\n",
    "- Product Recommendations, recommendation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search in E-Commerce\n",
    "\n",
    "A good search mechanism positively impacts the conversion rate, which directly impacts the revenue of the e-retailer.\n",
    "\n",
    "The left-most section in E-Commerce depicts a set of filters (alternatively, “facets”) that allows the customer to guide their search in a way that matches their buying needs.\n",
    "\n",
    "These filters are the key that defines the faceted search. However, they may not always be readily available for all products. Some reasons for that are:\n",
    "- The seller didn’t upload all the required information while listing the product on the e-commerce website.\n",
    "- Some of the filters are difficult to obtain, or the seller may not have the complete information to provide—for example, the caloric value of a food product, which is typically derived from the nutrient information provided on the product case.\n",
    "\n",
    "Faceted search can be built with most popular search engine backends like Solr and Elasticsearch.\n",
    "\n",
    "> TIP: In an e-commerce setting, we also need to account for business needs other than relevance in terms of facets and text. For instance, products that are part of a promotion or sale may be bumped up in results. This can be built by utilizing features like Elasticsearch boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an E-Commerce Catalog\n",
    "\n",
    "- Attribute Extraction, extract the attributes of product such as color, size, etc.\n",
    "    - Direct attribute extraction algorithms, assume the presence of the attribute value in the input text.\n",
    "    - Derived attribute extraction algorithms, do not assume that the attribute of interest is present in the input text.\n",
    "\n",
    "> TIP: For the models that use deep recurrent structures, the amount of data needed is typically much more than what’s needed when less-complex ML models such as CRF and HMM are used. The more data there is, the better the deep models learn. This is common to all DL models, as we saw in earlier chapters, but for e-commerce, getting a large set of well-sampled, annotated data is very expensive. Hence, it needs to be taken care of before we to build any sophisticated models.\n",
    "\n",
    "- Product Catagorization and Taxonomy\n",
    "\n",
    "A good taxonomy and properly linked products can be critical because it allows an e-commerce site to:\n",
    "1. Show products similar to the product searched\n",
    "2. Provide better recommendations\n",
    "3. elect appropriate bundles of products for better deals for the customer\n",
    "4. Replace old products with new ones\n",
    "5. Show price comparisons of different products in the same category\n",
    "\n",
    "There APIs that can build on large catalog content of various big retailers and provide the intelligence inside to categorize a product by scanning its unique product code such as Semantics3, eBay, and Lucidworks.\n",
    "\n",
    "- Product Enrichment, typically seen as a larger and more continuous process than just improving product titles in any online retail setup.\n",
    "- Product Deduplication and Matching\n",
    "    - Attribute Match, If two products are the same, then the values of various attributes must be the same. Hence, once the attributes are extracted, we compare values for attributes for both of the products in question.\n",
    "    - Title Match, we can compare bigrams and trigrams among titles duplicated.\n",
    "    - Image Match, pixel-to-pixel match, feature map matching, or even advanced image-matching techniques like Siamese networks are popular.\n",
    "    \n",
    "> TIP: A/B testing is a good method of measuring the results and effectiveness of different algorithms in the e-commerce world. For procedures like attribute extraction, product enrichment and A/B testing different models will lead to an impact on business metrics. These metrics can be direct or indirect sales, click-through rates, time spent on one web page, etc., and an improvement in relevant metrics shows that a model works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Analysis\n",
    "\n",
    "- Sentiment Analysis\n",
    "\n",
    "Negative reviews are more important to understand.\n",
    "\n",
    "> TIP: Typically, a review contains more than one sentence. It’s advisable to break a review into sentences and pass each sentence as one data point. This is also relevant for sentence-wise aspect tagging, aspect-wise sentiment analysis, etc.\n",
    "\n",
    "- Aspect-Level Sentiment Analysis, An aspect is a semantically rich, concept-centric collection of words that indicates certain properties or characteristics of the product. For example, aspects of a travel website might have: location, value, and cleanliness.\n",
    "    - Supervised Approach, depends mainly on seed words.\n",
    "    - Unsupervised Approach, topic modeling is a useful technique in identifying latent topics present in a document.\n",
    "    \n",
    "Connecting Overall Ratings to Aspects, we use a technique called latent rating regression analysis (LARA).\n",
    "\n",
    "> TIP: User information is also key in handling reviews. Imagine a scenario where a popular user, as opposed to a less-popular user, writes a good review. The user matters! While performing the review analysis, a “user weight” can be defined for all users based on their ratings (generally given by other peers) and can be used in all calculations to discount the reviewer bias.\n",
    "\n",
    "Understanding Aspects,\n",
    "\n",
    "Given the huge volume of reviews an e-commerce website encounters, there will still be a lot of sentences under an aspect. Here, a summarization algorithm may save the day. LexRank is an algorithm, similar to PageRank, that assumes each sentence is a node and connects via sentence similarity. Once done, it picks the most central sentences out of it and presents an extractive summary of the sentences under an aspect.\n",
    "\n",
    "Flowchart of review analysis: overall sentiments, aspect-level sentiments, and aspect-wise significant reviews.\n",
    "\n",
    "![alt text](https://learning.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0917.png)\n",
    "\n",
    "> TIP: A complete understanding of a product can only be achieved by both user reviews and editorial reviews. Editorial reviews are generally provided by expert users or domain experts. These reviews are more reliable and can be shown at the top of the review section. But on the other hand, general user reviews reveal the true picture of the product experience from all users’ perspectives. Hence, melding editorial reviews with general user reviews is important. That may be achieved by mixing both kinds of reviews in the top section and ranking them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for E-Commerce\n",
    "\n",
    "Comprehensive study of techniques for various e-commerce recommendation scenarios\n",
    "\n",
    "![alt text](https://learning.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0918.png)\n",
    "\n",
    "> NOTE: Recommendation engines deal with information from various sources. Proper matching of various data tables and consistency of the information across various data sources is important to maintain. For example, while collating the information about product attributes and product transaction history, the consistency of the information should be checked carefully. Complementary and substitute data can give indications about data quality. One should check for anomalous behavior while working with multifarious data sources, as in the case of e-commerce recommendation.\n",
    "\n",
    "### A Case Study: Substitutes and Complements\n",
    "\n",
    "Complements are products that are typically bought together. On the other hand, there are pairs that are bought in lieu of the other, and they’re known as substitute pairs.\n",
    "\n",
    "![alt text](https://learning.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0919.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra\n",
    "\n",
    "### LATENT ATTRIBUTE EXTRACTION FROM REVIEWS\n",
    "\n",
    "Reviews contain specific information about product attributes. Explicit extraction of attributes from reviews may have limitations in representation, as we need to define an explicit ontology, so instead, we learn them via a latent vector representation. \n",
    "\n",
    "### PRODUCT LINKING\n",
    "\n",
    "The next task is to understand how the two products are linked. We already obtained topic vectors, which capture the intrinsic properties of the product in a latent attribute space. Now, given a pair of products, we want to create a combined feature vector out of the respective topic vectors for the products and then predict if there’s any relationship between them. This can be viewed as a binary classification problem where the features have to be obtained from the respective topic vectors for the product pair. We call this process “link prediction”. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yasir Abdur\n",
      "[nltk_data]     Rohman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Yasir Abdur\n",
      "[nltk_data]     Rohman\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = \"This fried chicken tastes very good. It is juicy and perfectly cooked.\"\n",
    "negative = \"This fried chicken tasted bad. It is dry and overcooked.\"\n",
    "ambiguous = \"Except the amazing fried chicken everything else at the restaurant tastes very bad.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaderSentiment\n",
    "\n",
    "VADER Sentiment Analysis: VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:\n",
      "'This fried chicken tastes very good. It is juicy and perfectly cooked.'\n",
      "{'compound': 0.8122, 'neg': 0.0, 'neu': 0.575, 'pos': 0.425}\n",
      "------------------------------\n",
      "Negative:\n",
      "'This fried chicken tasted bad. It is dry and overcooked.'\n",
      "{'compound': -0.5423, 'neg': 0.28, 'neu': 0.72, 'pos': 0.0}\n",
      "------------------------------\n",
      "Ambiguous:\n",
      "('Except the amazing fried chicken everything else at the restaurant tastes '\n",
      " 'very bad.')\n",
      "{'compound': 0.0018, 'neg': 0.204, 'neu': 0.592, 'pos': 0.204}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# It scores from -1 to 1. -1 being negative and 1 being positive\n",
    "def sentiment_analyzer_scores(text):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    score = sentiment_analyzer.polarity_scores(text)\n",
    "    pprint(text)\n",
    "    pprint(score)\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "print(\"Positive:\")\n",
    "sentiment_analyzer_scores(positive)\n",
    "\n",
    "print(\"Negative:\")\n",
    "sentiment_analyzer_scores(negative)\n",
    "\n",
    "print(\"Ambiguous:\")\n",
    "sentiment_analyzer_scores(ambiguous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This fried chicken tastes very good. It is juicy and perfectly cooked.\n",
      "Positive: ['good', 'perfectly']\n",
      "Negative: []\n",
      "Neutral: ['This', 'fried', 'chicken', 'tastes', 'very', '.', 'It', 'is', 'juicy', 'and', 'cooked', '.']\n",
      "------------------------------\n",
      "This fried chicken tasted bad. It is dry and overcooked.\n",
      "Positive: []\n",
      "Negative: ['bad']\n",
      "Neutral: ['This', 'fried', 'chicken', 'tasted', '.', 'It', 'is', 'dry', 'and', 'overcooked', '.']\n",
      "------------------------------\n",
      "Except the amazing fried chicken everything else at the restaurant tastes very bad.\n",
      "Positive: ['amazing']\n",
      "Negative: ['bad']\n",
      "Neutral: ['Except', 'the', 'fried', 'chicken', 'everything', 'else', 'at', 'the', 'restaurant', 'tastes', 'very', '.']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_word_sentiment(text):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    \n",
    "    positive_words=[]\n",
    "    neutral_words=[]\n",
    "    negative_words=[]\n",
    "    for word in tokenized_text:\n",
    "        if (sentiment_analyzer.polarity_scores(word)['compound']) >= 0.1:\n",
    "            positive_words.append(word)\n",
    "        elif (sentiment_analyzer.polarity_scores(word)['compound']) <= -0.1:\n",
    "            negative_words.append(word)\n",
    "        else:\n",
    "            neutral_words.append(word)\n",
    "    print(text)\n",
    "    print('Positive:',positive_words)        \n",
    "    print('Negative:',negative_words)    \n",
    "    print('Neutral:',neutral_words)\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "get_word_sentiment(positive)\n",
    "get_word_sentiment(negative)\n",
    "get_word_sentiment(ambiguous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford Core NLP\n",
    "\n",
    "Before moving on to execute the code we need to start the Stanford Core NLP server on our local machine.\n",
    "To do that follow the steps below (tested on debian should work fine for other distributions too):\n",
    "\n",
    "1. Download the Stanford Core NLP model from [here](https://stanfordnlp.github.io/CoreNLP/#download).\n",
    "2. Unzip the folder\n",
    "3. cd into the folder\n",
    "4. cd stanford-corenlp-4.0.0/\n",
    "5. Start the server using this command:\n",
    "\n",
    "```java -mx5g -cp \"./*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000```\n",
    "\n",
    "If you do not have java installed on your system please install it from the official Oracle page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "def get_sentiment(text):\n",
    "    res = nlp.annotate(text,\n",
    "                       properties={'annotators': 'sentiment',\n",
    "                                   'outputFormat': 'json',\n",
    "                                   'timeout': 1000,\n",
    "                       })\n",
    "    print(text)\n",
    "    print('Sentiment:', res['sentences'][0]['sentiment'])\n",
    "    print('Sentiment score:', res['sentences'][0]['sentimentValue'])\n",
    "    print('Sentiment distribution (0-v. negative, 5-v. positive:', res['sentences'][0]['sentimentDistribution'])\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This fried chicken tastes very good. It is juicy and perfectly cooked.\n",
      "Sentiment: Negative\n",
      "Sentiment score: 1\n",
      "Sentiment distribution (0-v. negative, 5-v. positive: [0.14784497645863, 0.42150440150006, 0.26795232397677, 0.14973929036748, 0.01295900769705]\n",
      "------------------------------\n",
      "This fried chicken tasted bad. It is dry and overcooked.\n",
      "Sentiment: Verynegative\n",
      "Sentiment score: 0\n",
      "Sentiment distribution (0-v. negative, 5-v. positive: [0.59929214372637, 0.37183747332338, 0.00491027284258, 0.01321244183821, 0.01074766826945]\n",
      "------------------------------\n",
      "Except the amazing fried chicken everything else at the restaurant tastes very bad.\n",
      "Sentiment: Negative\n",
      "Sentiment score: 1\n",
      "Sentiment distribution (0-v. negative, 5-v. positive: [0.14730433605028, 0.42068997423614, 0.26920777577949, 0.14985314657637, 0.01294476735772]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(positive)\n",
    "get_sentiment(negative)\n",
    "get_sentiment(ambiguous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
